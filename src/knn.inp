function bundle set_bundle_and_get_defaults (const list xlist,
                                             bundle opts[null])
    /* Compile self bundle by merging eventual information
    from 'opts' bundle. */

    if !exists(opts)
        bundle opts = defbundle()
    endif

    bundle self = default_values()
    self = opts + self          # override defaults

    return self
end function


function bundle default_values (void)
    /*
    * This function creates a bundle with default values for the k-nearest neighbors
    * (knn) algorithm.
    *
    * Returns:
    *   A bundle containing the default values for the knn algorithm.
    */

    bundle self = defbundle()
    scalar self.stdize_features = TRUE
    string self.distance_type = "euclidean"
    string self.class_prediction = "majority"
    string self.splitters = "none"  # "stratified_kfold", "tscv", "loo"
    scalar self.kfold_nsplits = 5   # number of folds for kfold-splitters
    scalar self.win_size = 10   # number of folds for timeseries-splitters
    string self.scoring_regression = "rmse"
    string self.scoring_classification = "PRC"  # FSC=F1-Score
    scalar self.nobs = $nobs
    scalar self.sample_t1 = $t1
    scalar self.sample_t2 = $t2

    return self
end function


function matrix compute_distances (const matrix features,
                                   const string distance_type)
    /*
        Calculates the pairwise distances between the given features.

        Parameters:
            features: A matrix containing the features in columns and records in rows (=T).
            distance_type: string name of distance type

        Returns:
            D: T by T matrix containing the pairwise distances between the features.
            The rows refer to the i-th feature and the columns to the distances
            to the j-th feature. The diagonal contains the distances to itself is
            zero by definition.
    */
    matrix out = distance(features, distance_type)
    matrix D = unvech(out, 0)

    return D
end function


function bundle knn_fit (const series y "Target",
                         const list xlist "Features",
                         const numeric n_neighbors "No. of neighbors",
                         bundle opts[null] "Parameter bundle")
    /* Function for computing KNN: run the setup and train the model. */

    list L = y xlist
    errorif(sum(missing(L)), "Some  have missing values. Please drop these first.")
    errorif($nobs <= n_neighbors, "Number of rows must exceed number of neighbors.")
    errorif(typename(n_neighbors) != "scalar" && typename(n_neighbors) != "matrix",
            "Parameter 'n_neighbors' must bei either of type scalar or matrix.")
    errorif(isconst(y) == TRUE, "Dependent variable is constant. Abort.")

    if !exists(opts)
        bundle opts = defbundle()
    endif

    bundle self = set_bundle_and_get_defaults(xlist, opts)
    self.distance_type = tolower(self.distance_type)
    string self.depvar = argname(y)
    strings self.parnames = varnames(xlist)
    string self.type = getinfo(y).discrete == TRUE ? "classification" : "regression"
    string self.metric = get_metric_name(self)

    errorif(self.class_prediction == "probability", "Computing probabilities not yet supportted.")
    check_metric_probclass(self.type, self.metric, self.class_prediction)

    errorif_classification_loo(self.type, self.splitters)
    errorif_multiclass_prob(y, self.class_prediction, self.type)

    matrix target = {y}
    matrix mX = {xlist}
    matrix neighbor_values = values(vec(n_neighbors))
    scalar n_neighbor_values = nelem(neighbor_values)

    if self.splitters == "none"
        if self.type == "regression"
            matrix self.ess = mshape(NA, n_neighbor_values, 1)
            matrix self.rsq = mshape(NA, n_neighbor_values, 1)
            strings rlabel_rsq_ess = array(n_neighbor_values)
            string clabel_rsq = "R^2"
            string clabel_ess = "ess"
        endif
    else
        # each page refers to different number of folds
        matrices self.Scores = array(n_neighbor_values)
    endif

    loop nb=1..n_neighbor_values
        #printf "\nStart analysis for k = %d neighbors.\n", neighbor_values[nb]
        scalar self.n_neighbors = neighbor_values[nb]

        if self.splitters == "none"
            knn_compute(target, mX, &self)
            matrix self.uhat = self.target - self.yhat

            if self.type == "regression"
                scalar self.rsq[nb] = get_rsq(self.target, self.yhat)
                scalar self.ess[nb] = sst(self.uhat)
                rlabel_rsq_ess[nb] = sprintf("k=%d", neighbor_values[nb])
            elif self.type == "classification"
                # TODO: implement support for classification?
            endif
        else
            matrix self.features = mX
            bundle model = self
            knnCV(target, mX, &model)
            self.Scores[nb] = model.Scores
            if nb == 1
                self.n_training_sets = model.n_training_sets
            endif
        endif
        #printf "\nFinished analysis for k = %d neighbors.\n", neighbor_values[nb]
    endloop

    # Overwrite the temporary scalar value by the matrix holding all distinct values evaluated
    delete self.n_neighbors
    self.n_neighbors = n_neighbors

    if inbundle(self, "rsq")
        cnameset(self.rsq, clabel_rsq)
        rnameset(self.rsq, rlabel_rsq_ess)
    endif
    if inbundle(self, "ess")
        cnameset(self.ess, clabel_ess)
        rnameset(self.ess, rlabel_rsq_ess)
    endif

    if inbundle(self, "Scores")
        matrix self.mean_scores = get_scores_by_neighbor(self.Scores)
        bundle Btmp = get_optimal_score_and_k(self.mean_scores, self.metric, self.type)
        self.optimal_k = Btmp.optimal_k
        self.optimal_score = Btmp.optimal_score
    endif

    return self
end function


function void knn_plot_score (const bundle self,
                              const string filename[null])
    /*
    * Plots the optimal score for different values of k in k-nearest neighbors algorithm.
    *
    * Parameters:
    *    self: bundle
    *        The bundle containing the mean scores and other information.
    *    filename: string (optional)
    *        The name of the output file. If not provided, the plot will be displayed.
    *
    * Notes:
    *    - The function requires cross-validation results in the bundle.
    *    - The plot will show the optimal score for each value of k.
    *    - The x-axis represents the number of neighbors (k), and the y-axis represents the mean score.
    *    - The title of the plot includes the optimal score, the optimal k value, and the cross-validation method used.
    *    - The plot will be displayed or saved to a file based on the provided filename.
    */

    if inbundle(self, "mean_scores") && nelem(self.n_neighbors) > 1
        if self.type == "regression"
            string metric = toupper(self.scoring_regression)
        elif self.type == "classification"
            string metric = toupper(self.scoring_classification)
        endif
        scalar pos = instrings(cnameget(self.mean_scores), metric)
        matrix toplot = self.mean_scores[,pos] ~ vec(uniq(self.n_neighbors))
        string fname = exists(filename) ? filename : "display"
        string title = sprintf("Optimal score of %.4f for k = %d, cv: %s",
                               self.optimal_score, self.optimal_k, self.splitters)

        gnuplot --matrix=toplot --with-lines --output="@fname" \
            {set grid; set xlabel "k neighbors" font ",13";\
             set ylabel "mean of @metric" font ",13";\
             set title "@title" font ",13";\
             set linetype 1 lw 2; }
    else
        print "WARNING: Plot requires cross-validation results and/or multiple neighbors to evaluate."
    endif
end function


function void knn_plot_cvscores (const bundle self,
                                 const string filename[null])
    /*
    * Plots the distribution of cross-validated scores for different values of k in k-nearest neighbors algorithm.
    *
    * Parameters:
    *    self: bundle
    *        The bundle returned by model fitting.
    *    filename: string (optional)
    *        The name of the output file. If not provided, the plot will be displayed.
    *
    * Notes:
    *    - The function requires cross-validation results in the bundle.
    *    - The plot will show a boxplot of the distribution of the scores across folds for each value of k.
    *    - The x-axis represents the number of neighbors (k), and the y-axis represents the score.
    *    - The plot will be displayed or saved to a file based on the provided filename.
    */

    if inbundle(self, "Scores")
        if self.type == "regression"
            string metric = toupper(self.scoring_regression)
        elif self.type == "classification"
            string metric = toupper(self.scoring_classification)
        endif
        scalar pos = instrings(cnameget(self.mean_scores), metric)
        # Retrieve cv scores for each no. of neighbors
        matrix scores = drill(self.Scores, null, pos)  # rows=folds, cols: n-neighbors
        cnameset(scores, get_neighbors_label(nelem(self.n_neighbors)))
        string fname = exists(filename) ? filename : "display"
        string title = sprintf("Cross-validated scores, Optimal k=%d based on '%s' ('%s')",
                               self.optimal_k, metric, self.splitters)

        boxplot --matrix=scores --output="@fname" \
            {set grid; set xlabel "neighbors" font ",13";\
             set ylabel "Distribution of @metric" font ",13";\
             set title "@title" font ",13";\
             set linetype 1 lw 2; }
    else
        print "WARNING: Plot requires cross-validation results."
    endif
end function


function strings get_folds_label (const int nfolds)
    /*
    * Generates an array of strings representing the labels for each fold in a cross-validation process.
    *
    * Parameters:
    *   nfolds - The number of folds in the cross-validation process.
    *
    * Returns:
    *   An array of strings, where each string represents the label for a fold.
    */

    strings S = array(nfolds)
    loop i=1..nfolds
        S[i] = sprintf("fold=%d", i)
    endloop

    return S
end function



function strings get_neighbors_label (const int n_neighbors)
    /*
    * This function generates an array of strings representing the labels for the nearest neighbors.
    *
    * Parameters:
    *   - n_neighbors: The number of nearest neighbors.
    *
    * Returns:
    *   - An array of strings representing the labels for the nearest neighbors.
    */

    strings S = array(n_neighbors)
    loop i=1..n_neighbors
        S[i] = sprintf("k=%d", i)
    endloop

    return S
end function



function matrix get_scores_by_neighbor (const matrices Scores)
    /*
    * This function calculates the mean score for each neighbor in a set of scores.
    *
    * Parameters:
    * - Scores: A set of matrices containing scores for each neighbor. Each column refers
    *  to some metric and each row to some fold on which the model was trained on.
    *
    * Returns:
    * - mean_score_by_neighbor: A matrix containing the mean score for each neighbor.
    */

    scalar ncols = cols(Scores[1])
    scalar npages = nelem(Scores)
    strings clabels = cnameget(Scores[1]) # retrieve metric names
    matrix mean_score_by_neighbor = mshape(NA, npages, ncols)

    loop i=1..ncols
        matrix mean_score_by_neighbor[,i] = meanc(drill(Scores, , i))'
    endloop

    cnameset(mean_score_by_neighbor, clabels)
    rnameset(mean_score_by_neighbor, get_neighbors_label(npages))

    return mean_score_by_neighbor
end function


function void knn_compute (const matrix target,
                           const matrix features,
                           bundle *self,
                           const bool compute_yhat[TRUE])
    /*
    * This function computes the k-nearest neighbors (KNN) algorithm.
    *
    * Parameters:
    * - target: The matrix of target variables.
    * - features: The matrix of features.
    * - self: A bundle containing additional parameters and variables.
    * - compute_yhat: A boolean indicating whether to compute the predicted values.
    *
    * Returns:
    * This function does not return any value.
    *
    * Description:
    * The function computes the KNN algorithm by finding the k nearest neighbors for each observation in the features matrix.
    * It then computes the predicted values for the target variables based on the type of estimation (regression or classification).
    * The computed values are stored in the self.yhat matrix if compute_yhat is set to TRUE.
    */

    matrix self.target = target
    matrix self.features = prepare_features(features, self.stdize_features)
    matrix distances = compute_distances(self.features, self.distance_type)
    # Compute the indices for each observation which are the closest to it
    matrix indices_of_closest = indices_of_closest(distances, self.n_neighbors)

    if compute_yhat == TRUE
        # Compute the predictions for the target using data on the closest neighbors
        if self.type == "regression"
            matrix self.yhat = compute_yhat_regression(target, indices_of_closest)
        elif self.type == "classification"
            matrix self.yhat = compute_yhat_classification(target,
                                                           indices_of_closest,
                                                           self.class_prediction)
        else
            errorif(TRUE, "unknown estimation class")
        endif
    endif
end function



function void knnCV (const matrix target, const matrix features, bundle *self)
    /*
    * This function performs k-fold cross-validation for the k-nearest neighbors (k-NN) algorithm.
    * It takes a target matrix and a features matrix as input, along with a bundle containing
    * various parameters for the k-NN algorithm. The function splits the data into training and
    * validation sets using a specified cross-validation method, and then estimates the k-NN model
    * for each training set. Finally, it computes the scores for each validation set and returns
    * the results as a matrix.
    *
    * Parameters:
    *    target: The target matrix containing the dependent variable.
    *    features: The features matrix containing the independent variables.
    *    self: A bundle containing the following parameters for the k-NN algorithm:
    *        - kfold_nsplits: The number of folds for cross-validation.
    *        - win_size: The window size for the time-series cross-validation method.
    *        - splitters: The type of cross-validation method to use.
    *        - stdize_features: Whether to standardize the features before fitting the model.
    *        - type: The type of k-NN algorithm to use (e.g., classification or regression).
    *        - class_prediction: The type of class prediction to use for classification.
    *        - n_neighbors: The number of nearest neighbors to consider.
    *        - distance_type: The type of distance metric to use.
    *
    * Returns:
    *    The scores matrix, which contains the evaluation scores for each validation set.
    */

    # Preparations for CvDataSplitter
    bundle DataCV = null
    matrix DataCV.X = target ~ features
    matrix DataCV.index = seq(1, rows(DataCV.X))'
    scalar DataCV.n_folds = self.kfold_nsplits
    scalar DataCV.win_size = self.win_size
    string DataCV.cv_type = self.splitters
    # Retrive the training and validation data
    CvDataSplitter(&DataCV)
    scalar self.n_training_sets = nelem(DataCV.X_train)

    # Estimate the model for each training set
    matrix Scores = {}

    loop tset=1..self.n_training_sets
        bundle model = _(stdize_features = self.stdize_features,
                         type = self.type,
                         class_prediction = self.class_prediction,
                         n_neighbors = self.n_neighbors,
                         distance_type = self.distance_type)
        matrix y_train = DataCV.X_train[tset][,2]  # 1st column refers to index
        matrix X_train = DataCV.X_train[tset][,3:]
        matrix y_test = DataCV.X_test[tset][,2] # 1st column refers to index
        matrix X_test = DataCV.X_test[tset][,3:]
        assert(rows(y_train) == rows(X_train))
        assert(rows(y_test) == rows(X_test))

        # set computing in-sample predictions to FALSE as not needed here
        knn_compute(y_train, X_train, &model, FALSE)
        # predict using the validation set
        matrix forecast = knn_predict(model, X_test)
        matrix Scores ~= knn_scores(y_test, forecast, model)
        strings metric_labels = tset == 1 ? rnameget(Scores) : metric_labels
    endloop

    self.Scores = Scores'
    rnameset(self.Scores, get_folds_label(tset))
    cnameset(self.Scores, metric_labels)
end function


function bundle get_optimal_score_and_k (const matrix mean_scores,
                                         const string metric,
                                         const string type)
    /* Based on cross-validated mean scores, retrieve the optimal number of
    neighbors based on some metric. */

    catch scalar position_criteria = instrings(cnameget(mean_scores), toupper(metric))
    errorif($error, "Unknown metric name passed.")

    bundle B = empty
    if type == "classification"
        matrix corrected_mean_scores = correct_class_metrics(mean_scores[,position_criteria],
                                                             metric)
    else
        matrix corrected_mean_scores = mean_scores[,position_criteria]
    endif
    scalar B.optimal_k = iminc(corrected_mean_scores, TRUE)
    scalar B.optimal_score = NA

    if !ok(B.optimal_k)
        printf "ERROR: Cannot compute optimal no. of neighbors for metric '%s'.\n", metric
        print "Please select another metric instead."
        return B
    endif

    scalar B.optimal_score = mean_scores[B.optimal_k, position_criteria]

    return B
end function


function void knn_summary (const bundle self)
    /* Print a summary of the KNN model. */

    printf "\nK-Nearest Neighbors Model Summary\n"
    printf "----------------------------------\n"
    printf "Type:                       %s\n", self.type
    if self.splitters == "none"  # no CV performed
        if typename(self.n_neighbors) == "scalar"
            printf "Number of neighbors:        %d\n", self.n_neighbors
        else
            printf "Number of neighbors:        %d to %d\n",
              min(self.n_neighbors), max(self.n_neighbors)
        endif
    else
        printf "Splitter:                   %s\n", self.splitters
        printf "No. of training sets        %d\n", self.n_training_sets
        printf "Optimal no. of neighbors:   %d\n", self.optimal_k
        printf "Score name:                '%s'\n", self.metric
        printf "Best mean score:            %.4f\n", self.optimal_score
    endif
    printf "Distance type:              %s\n", self.distance_type
    printf "Standardize features:       %s\n", self.stdize_features ? "Yes" : "No"
    printf "Target variable:            %s\n", self.depvar
    printf "Feature variables:          %s\n", flatten(self.parnames, ", ")
    printf "Number of observations:     %d\n", self.nobs
    printf "Sample period:              %d to %d\n", self.sample_t1, self.sample_t2
    if inbundle(self, "rsq") && typename(self.rsq) == "scalar"
        printf "R-squared:               %.4f\n", self.rsq
    endif
    if inbundle(self, "ess") && typename(self.ess) == "scalar"
        printf "Sum of squares:          %.4f\n", self.ess
    endif
    printf "----------------------------------\n"
end function


function matrix knn_predict (const bundle self,
                             const numeric X)
    /*
        This function performs k-nearest neighbors prediction on new data points.

        Parameters:
            - self: The bundle containing the training data and model parameters.
            - X: The new data points to be predicted.

        Returns:
            - prediction: The predicted values for the new data points.

        Notes:
            - The function checks if the number of columns in the new data points matches the number of columns in the training data.
            - The function prepares the new data points by standardizing the features if required.
            - The function computes the distances between the new data points and the training data.
            - The function selects the closest neighbors for each new data point.
            - The function computes the predicted values based on the type of problem (regression or classification).
    */

    if typename(X) == "series" || typename(X) == "list"
        matrix new_data = {X}
    else
        matrix new_data = X
    endif

    errorif(cols(new_data) != cols(self.features),
            "The number of columns in the new data points does not match the number of columns in the training data.")
    scalar nrows_X = rows(new_data)
    scalar ncols_X = cols(new_data)
    matrix new_data = prepare_features(new_data, self.stdize_features)
    matrix new_data |= self.features
    matrix distances = compute_distances(new_data, self.distance_type)
    # Remove the initial nrows_X columns which are the distances to the test data
    # Keep only the initial nrows_X rows which are the distances for the nrows_X testdata records
    distances = distances[1:nrows_X,-seq(1,nrows_X)]
    assert(rows(distances) ==  nrows_X)
    assert(cols(distances) == rows(self.features))

    # Compute the indices for each training record which are the closest to the test record
    matrix indices_of_closest = indices_of_closest(distances, self.n_neighbors)

    # Compute the predictions for the target using data on the closest neighbors
    if self.type == "regression"
        matrix prediction = compute_yhat_regression(self.target, indices_of_closest)
    else
        matrix prediction = compute_yhat_classification(self.target,
                                                        indices_of_closest,
                                                        self.class_prediction)
    endif

    return prediction
end function



function matrix knn_scores (const numeric actual,
                            const numeric pred,
                            const bundle self)
    /*
    Calculates the evaluation statistics for the k-nearest neighbors (kNN) algorithm.

    Parameters:
        actual (numeric): The actual target variable values. Can be a series or a column vector.
        pred (numeric): The predicted target variable values. Can be a series or a column vector.
        self (bundle): A bundle containing additional information about the algorithm.

    Returns:
        fcstats (matrix): The forecast statistics matrix.

    Notes:
        - The target variable must be either a series or a column vector.
        - The prediction variable must be either a series or a column vector.
        - The bundle 'self' must contain the following properties:
            - type (string): The type of the algorithm ('regression' or 'classification').
            - class_prediction (string): The method used for class prediction in case of classification ('majority' or 'probability').
    */

    if typename(actual) == "list" || typename(pred) == "list"
        errorif(TRUE, "The actual and prediction variable must be either a series or a column vector.")
    endif

    if typename(actual) == "series"
        matrix target = {actual}
    else
        matrix target = actual
    endif

    if typename(pred) == "series"
        matrix prediction = {pred}
    else
        matrix prediction = pred
    endif

    if self.type == "regression"
        matrix fcstats = fcstats(target, prediction)
    elif self.type == "classification"
        if self.class_prediction == "majority"
            matrix fcstats = fcstats_majority(target, prediction)
        elif self.class_prediction == "probability"
            matrix fcstats = fcstats_probability(target, prediction)
        endif
    else
        errorif(TRUE, "unknown estimation class")
    endif

    return fcstats
end function



function matrix fcstats_majority (const matrix actual,
                                  const matrix prediction)
    /**
    * Compute evaluation statistics for binary integer outcomes using the majority rule.
    *
    * actual: The matrix of true actual values.
    * prediction: The matrix of predicted values.
    * return: The matrix of evaluation statistics, including false rate, hit rate, and Kuipers score.
    */

    matrix scores = scores2x2(actual ~ prediction, FALSE)

    return scores
end function

function matrix fcstats_probability (const matrix actual,
                                     const matrix prediction)
    /*
    * This function computes evaluation statistics for binary classification models.
    *
    * Parameters:
    * - actual: A matrix containing the actual class labels.
    * - prediction: A matrix containing the predicted probabilities for each class.
    *
    * Returns:
    * - fcstats: A matrix containing the evaluation statistics (computed by the "FEP" package).
    *   - If the number of unique values in 'actual' is 2, the column vector
    *     contains the following statistics:
    *     - Quadratic probability score (QPS)
    *     - Logarithmic probability score (LPS)
    *   - If the number of unique values in 'actual' is not 2, a warning message is printed and an empty matrix is returned.
    */

    if nelem(values(actual)) == 2
        matrix fcstats = probscore(actual, prediction)
        strings label = cnameget(fcstats)
        fcstats = fcstats'
        rnameset(fcstats, label)
    else
        printf "\nWARNING: No support for statistics for more than three class outcomes.\n\n"
        matrix fcstats = {}
    endif

    return fcstats
end function


function matrix prepare_features (matrix features,
                                  const bool stdize_features)
    /*
    * This function prepares the features matrix for k-nearest neighbors (KNN) algorithm.
    *
    * Parameters:
    *   - features: A matrix containing the features.
    *   - stdize_features: A boolean indicating whether to standardize the features.
    *
    * Returns:
    *   - A matrix containing the prepared features.
    */
    if stdize_features && rows(features) > 1
        matrix features = stdize(features, 0, TRUE)
    endif

    return features
end function


function scalar get_rsq (const matrix target, const matrix yhat)
    /*
    * Calculates the R-squared value between the target and predicted values.
    *
    * target: The matrix of target values.
    * yhat: The matrix of predicted values.
    * return: The R-squared value.
    */
    scalar rsq = mcorr(target~yhat)[1,2]^2
    return rsq
end function


function matrix compute_yhat_regression (const matrix target,
                                         const matrix indices_of_closest)
    /*
    * Compute the predicted values (yhat) based on the target matrix and
    * the indices of the closest neighbors for regressions.
    *
    * target: The target matrix containing the true values.
    * indices_of_closest: The matrix containing the indices of the closest neighbors.
    *
    * return: The matrix containing the predicted values (yhat).
    */

    matrix yhat = target_means(target, indices_of_closest)
    return yhat
end function


function matrix compute_yhat_classification (const matrix target,
                                             const matrix indices_of_closest,
                                             const string class_prediction)
    /*
    * Compute the predicted values (yhat) based on the target matrix and
    * the indices of the closest neighbors for classification.
    *
    * target: The target matrix containing the true values.
    * indices_of_closest: The matrix containing the indices of the closest neighbors.
    *
    * return: The matrix containing the predicted values (yhat).
    */

    matrix yhat = target_class(target, indices_of_closest, class_prediction)

    return yhat
end function


function matrix target_means (const matrix target,
                              const matrix indices_of_closest)
    /*
    * Calculates the mean values of the target variable from the training data
    * for each observation of the closest neighbors (measured relative to the test data
    * record).
    *
    * target: The matrix containing the target variable values from the training data.
    * indices_of_closest: The matrix containing the indices of the closest
      neighbors for each observation.

    * return: The matrix of mean values of the target variable for each observation.
    */

    scalar N = cols(indices_of_closest)
    matrix yhat = mshape(NA, N, 1)

    loop i=1..N
        matrix idx = indices_of_closest[,i]
        matrix yhat[i] = mean(target[idx])
    endloop

    return yhat
end function


function matrix target_class (const matrix target,
                              const matrix indices_of_closest,
                              const string class_prediction)
    /*
    * This function calculates either the mode or conditional probability of the target
    * variable for each observation based on the indices of the closest neighbors.
    *
    * Parameters:
    * - target: A matrix containing the target variable values for all observations.
    * - indices_of_closest: A matrix containing the indices of the closest
    *   neighbors for each observation.
    * - class_prediction: Type of prediction; either `majority` or `probability`.
    *
    * Returns:
    * - yhat: A matrix containing either the mode or conditional probability of
    *   the target variable for each observation.
    */

    scalar N = cols(indices_of_closest)
    matrix yhat = mshape(NA, N, 1)
    scalar information = class_prediction == "majority" ? 1 : 2

    loop i = 1..N
        matrix idx = indices_of_closest[,i]
        matrix value = onemode(target[idx])[information]

        if ok(value)
            matrix yhat[i] = value
        else
            printf "WARNING: Could not compute prediction for observation %d. Ignore.\n", $i
        endif
    endloop

    return yhat
end function


function matrix indices_of_closest (const matrix distances, const int n_neighbors)
    /*
    This function calculates the indices of the closest neighbors based on the given distances.

    Parameters:
        - distances: matrix, the distances between observations. The rows refer to
          each testdata record. The columns refer to the distance of each testdata
          observation to each point of the training set
        - n_neighbors: int, the number of closest neighbors to consider.

    Returns:
        - idx: matrix, the indices of the closest neighbors (in rows) for each
          testdata record (in columns). The rows equal the number of neighbors.
    */

    scalar N = rows(distances)
    matrix indices_of_closest = mshape(NA, n_neighbors, N)
    matrix idx = seq(1, cols(distances))'

    # TODO: Check whether this can be improved in terms of computation
    # TODO: In principle this may be parallelized but not in this version

    loop i = 1..N
        matrix row = distances[i,]' ~ idx
        # sort by distance in ascending order
        matrix sorted = msortby(row, 1)[-i,]  # remove reference observation
        matrix indices_of_closest[,i] = sorted[1:n_neighbors, 2]
    endloop

    return indices_of_closest
end function


function matrix correct_class_metrics (const matrix Scores,
                                       const string metric)
    /* This function computes the correct classification metrics by adjusting the
    sign of the scores based on predefined criteria. It is designed to work within
    the context of classification tasks, where different metrics might need to be
    interpreted differently (e.g., a higher score might be better for some metrics
    but worse for others). */

    bundle Map = _(POD = "negative",
                   POFD = "unchanged",
                   HR = "negative",
                   FAR = "unchanged",
                   CSI = "negative",
                   OR = "negative",
                   BIAS = "negative",
                   TSS = "negative",
                   HSS = "negative",
                   ETS = "unchanged",
                   PRC = "negative",
                   FSC = "negative",
                   QPS = "negative",
                   LPS = "negative")

    scalar pos = inbundle(Map, metric)
    errorif(!ok(pos), "Unknown metric name passed.")

    matrix ret = Scores .* (Map[metric] == "positive" ? 1 : -1)

    return ret
end function


function string get_metric_name (const bundle self)
    if self.type == "classification"
        string metric = self.scoring_classification
    else
        string metric = self.scoring_regression
    endif

    return metric
end function

function void check_metric_probclass (const string type,
                                      const string metric,
                                      const string class_prediction)

    if type == "classification"
        if metric == "QPS" && class_prediction != "probability"
            printf "\nERROR: The metric '%s' is only supported for probability-based classification.\n", metric
            errorif(TRUE, sprintf("Please set 'class_prediction' to 'probability' to use this metric.\n\n"))
        elif class_prediction == "probability" && (metric != "QPS" && metric != "LPS")
            errorif(TRUE, "For binary classification, only 'QPS' and 'LPS' are supported. Please use another metric")
        endif
    endif
end function

function void errorif_multiclass_prob (const series y,
                                       const string class_prediction,
                                       const string type)
    errorif(type == "classification" && nelem(values(y)) > 2 && \
            class_prediction == "probability",
            "Probabilities are only supported for binary classification, yet.")
end function

function void errorif_classification_loo (const string type,
                                            const string splitters)
    errorif(splitters == "loo" && type == "classification",
            "Leave-one-out CV (loo) not supportet for classification, yet. Please use 'kfold', instead.")
end function
