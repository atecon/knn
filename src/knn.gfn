<?xml version="1.0" encoding="UTF-8"?>
<gretl-functions>
<gretl-function-package name="knn" minver="2024a">
<author email="atecon@posteo.de">Artur Tarassow</author>
<version>0.1</version>
<date>2024-05-21</date>
<description>KNN regression and classification estimator</description>
<tags>C13 C52</tags>
<help filename="knn_help.md">
# KNN Package Documentation

The KNN (K-Nearest Neighbors) package is a powerful tool for performing KNN classification and regression tasks. It provides a set of functions that allow you to fit a model, make predictions, get scores, summarize the model, and plot the scores.

Please report any issues or suggestions on the [GitHub page](https://github.com/atecon/knn) or Gretl mailing list.


# Public Functions

### `knn_fit(train_data, train_labels, n_neighbors)`

This function fits the KNN model to the training data.

*Parameters:*

- `y`: *series*, The training data to fit the model on.
- `xlist`: *list*, The features to use for the KNN algorithm.
- `n_neighbors`: *int* or *matrix*, The number of neighbors to use for the KNN algorithm. If an integer is provided, a single KNN model is fitted. If a matrix is provided, multiple KNN models are fitted with different numbers of neighbors.
- `opts[null]`: *bundle*, Optionally, a bundle of options to pass to the KNN algorithm.

The `opts` bundle can contain the following options:

- `distance_type`: *string*, The distance metric to use for the KNN algorithm. Default is &quot;euclidean&quot;. Possible values are the ones supported by Gretl's built-in function `distance()` (see `help distance`).
- `class_prediction`: *string*, The method to use for predicting classes in a classification task. Default is &quot;majority&quot;. Possible values are

  + &quot;majority&quot;
  + &quot;probability&quot;.

  Majority returns the most common class among the neighbors, while probability returns the proportion of neighbors that belong to the class most common among the neighbors.

- `scoring_regression`: *string*, The method to use for scoring the model in a regression task. Default is &quot;rmse&quot;. Possible values are:

  + &quot;me&quot;

  + &quot;rmse&quot;

  + &quot;mae&quot;

  + &quot;mape&quot;

  + and others supported by Gretl's built-in function `fcstats()` (see `help fcstats`).

- `scoring_classification`: *string*, The method to use for scoring the model in a classification task. Default is &quot;FSC&quot; referring to the F1-score which balances recall and precision equally and reduces to the simpler equation 2TP/(2TP + FP + FN). Alternatives are:


  + &quot;POD&quot;: Prob. of detection

  + &quot;POFD&quot;: Prob. of false detection

  + &quot;HR&quot;: Hit rate

  + &quot;FAR&quot;: False alaram rate

  + &quot;CSI&quot;: Critical success index

  + &quot;OR&quot;: Odds ratio

  + &quot;BIAS&quot;: Bias score

  + &quot;TSS&quot;: Hanssen-Kuipers score (POD - POFD)

  + &quot;HSS&quot;: Heidke skill score

  + &quot;ETS&quot;: Equitable threat score

  + &quot;PRC&quot;: Precision

  + &quot;FSC&quot;: F-Score

- `splitters`: *string*, The method to use for splitting the data into training and test sets. Default is &quot;none&quot; implying that the data is not split and no cross-validation is performed. Possible values are:

  + &quot;kfold&quot;: perform k-fold cross-validation with the number of folds specified by the `kfold_nsplits` parameter (default: 5).

  + &quot;loo&quot;: perform leave-one-out cross-validation.

  + &quot;recwin&quot;: perform recursive window cross-validation with the window size specified by the &quot;win_size&quot; parameter (default: 10).

  + &quot;rolwin&quot;: perform rolling window cross-validation with the window size specified by the &quot;win_size&quot; parameter (default: 10).

- `stdize_features`: *bool*, Whether to standardize the features before fitting the model. Default is &quot;TRUE&quot;.


**Returns:**

A fitted KNN model object stored in a `bundle`. The bundle includes the following elements:

- `nobs`: *int*, Number of observations in the training and validation data.
- `optimal_k`: *int*, The optimal number of neighbors selected by the cross-validation procedure (only if cross-validation is performed).
- `optimal_score`: *scalar*, The optimal score achieved by the model on the validation data (only if cross-validation is performed).
- `sample_t1`: *int*, The index of the first observation in the training set.
- `sample_t2`: *int*, The index of the last observation in the training set.
- `features`: *matrix*, The features used for fitting the model.
- `mean_scores`: *matrix*, The mean scores achieved by the model on the validation data for each number of neighbors (only if cross-validation is performed). Rows represent the number of neighbors used, and columns represent the scoring metrics.
- `depvar`: *string*, The dependent variable used for fitting the model.
- `parnames`: *string array*, The names of the features used for fitting the model.
- `type`: *string*, The type of the model (classification or regression).
- `Scores`: *matrices*, Array of matrices containing the scores achieved by the model on the validation data for each number of neighbors (only if cross-validation is performed). Rows represent the k-fold splits, and columns represent the scoring metrics. Each matrix corresponds to a different number of neighbors.


### `knn_predict(model, X)`

This function uses a fitted KNN model to make predictions on the test data.

*Parameters:*

- `model`: *bundle*, The fitted KNN model object.
- `X`: *numeric*, A list or matrix of the test data to make predictions on.

**Returns:**

- A matrix of predictions.


### `knn_scores(actual, pred, model)`

This function calculates the accuracy score (for classification tasks) or the mean squared error (for regression tasks) of the KNN model.

*Parameters:*

- `actual`: *series* or *matrix*, The actual data.
- `pred`: *series* or *matrix*, The predicted data.
- `model`: The fitted KNN model.

**Returns:**

A *matrix* holding various accuracy scores.


### `knn_summary(model)`

This function provides a summary of the KNN model.

*Parameters:*

- `model`: *bundle*, The fitted KNN model.

**Returns:**

- A summary of the model.


### `knn_plot_score(model, filename[null])`

This function generates a plot showing the mean performance (across all cross-validation iterations) of the KNN model as a function of the number of neighbors.

*Parameters:*

- `model`: The fitted KNN model.
- `filename`: *string*, The name of the file to save the plot to. If not provided, the plot is displayed in the Gretl GUI.

**Returns:**

- A plot showing the model's performance.


# Change Log

- v0.1 (June 2024): Initial release.
</help>
<depends count="3">
extra FEP CvDataSplitter </depends>
<gretl-function name="knn_fit" type="bundle">
 <params count="4">
  <param name="y" type="series" const="true">
<description>Target</description>
  </param>
  <param name="xlist" type="list" const="true">
<description>Features</description>
  </param>
  <param name="n_neighbors" type="numeric" const="true">
<description>No. of neighbors</description>
  </param>
  <param name="opts" type="bundle" optional="true">
<description>Parameter bundle</description>
  </param>
 </params>
<code>/* Function for computing KNN: run the setup and train the model. */

list L = y xlist
errorif(sum(missing(L)), &quot;Some  have missing values. Please drop these first.&quot;)
errorif($nobs &lt;= n_neighbors, &quot;Number of rows must exceed number of neighbors.&quot;)
errorif(typename(n_neighbors) != &quot;scalar&quot; &amp;&amp; typename(n_neighbors) != &quot;matrix&quot;, &quot;Parameter 'n_neighbors' must bei either of type scalar or matrix.&quot;)

if !exists(opts)
  bundle opts = defbundle()
endif

bundle self = set_bundle_and_get_defaults(xlist, opts)
self.distance_type = tolower(self.distance_type)
string self.depvar = argname(y)
strings self.parnames = varnames(xlist)
string self.type = getinfo(y).discrete == TRUE ? &quot;classification&quot; : &quot;regression&quot;
if self.type == &quot;classification&quot;
  string self.metric = self.scoring_classification
else
  string self.metric = self.scoring_regression
endif
matrix target = {y}
matrix mX = {xlist}

matrix neighbor_values = values(vec(n_neighbors))
scalar n_neighbor_values = nelem(neighbor_values)

if self.splitters == &quot;none&quot;
  if self.type == &quot;regression&quot;
    matrix self.ess = mshape(NA, n_neighbor_values, 1)
    matrix self.rsq = mshape(NA, n_neighbor_values, 1)
    strings rlabel_rsq_ess = array(n_neighbor_values)
    string clabel_rsq = &quot;R^2&quot;
    string clabel_ess = &quot;ess&quot;
  endif
else
  matrices self.Scores = array(n_neighbor_values)
endif

loop nb=1..n_neighbor_values
  #printf &quot;\nStart analysis for k = %d neighbors.\n&quot;, neighbor_values[nb]
  scalar self.n_neighbors = neighbor_values[nb] # # add to self

  if self.splitters == &quot;none&quot;
    knn_compute(target, mX, &amp;self)
    matrix self.uhat = self.target - self.yhat

    if self.type == &quot;regression&quot;
      # TODO: also valid for classification?
      scalar self.rsq[nb] = get_rsq(self.target, self.yhat)
      scalar self.ess[nb] = sst(self.uhat)
      rlabel_rsq_ess[nb] = sprintf(&quot;k=%d&quot;, neighbor_values[nb])
    endif
  else
    matrix self.features = mX
    bundle model = self
    knnCV(target, mX, &amp;model)
    self.Scores[nb] = model.Scores
  endif

  #printf &quot;\nFinished analysis for k = %d neighbors.\n&quot;, neighbor_values[nb]
endloop

# overwrite the temporary value
delete self.n_neighbors
self.n_neighbors = n_neighbors

if inbundle(self, &quot;rsq&quot;)
  cnameset(self.rsq, clabel_rsq)
  rnameset(self.rsq, rlabel_rsq_ess)
endif
if inbundle(self, &quot;ess&quot;)
  cnameset(self.ess, clabel_ess)
  rnameset(self.ess, rlabel_rsq_ess)
endif

if inbundle(self, &quot;Scores&quot;)
  matrix self.mean_scores = get_scores_by_neighbor(self.Scores)
  bundle Btmp = get_optimal_score_and_k(self.mean_scores, self.metric)
  self.optimal_k = Btmp.optimal_k
  self.optimal_score = Btmp.optimal_score
endif

return self
</code>
</gretl-function>
<gretl-function name="knn_predict" type="matrix">
 <params count="2">
  <param name="self" type="bundle" const="true"/>
  <param name="X" type="numeric" const="true"/>
 </params>
<code>/*
This function performs k-nearest neighbors prediction on new data points.

Parameters:
- self: The bundle containing the training data and model parameters.
- X: The new data points to be predicted.

Returns:
- prediction: The predicted values for the new data points.

Notes:
- The function checks if the number of columns in the new data points matches the number of columns in the training data.
- The function prepares the new data points by standardizing the features if required.
- The function computes the distances between the new data points and the training data.
- The function selects the closest neighbors for each new data point.
- The function computes the predicted values based on the type of problem (regression or classification).
*/

if typename(X) == &quot;series&quot; || typename(X) == &quot;list&quot;
  matrix new_data = {X}
else
  matrix new_data = X
endif

errorif(cols(new_data) != cols(self.features), &quot;The number of columns in the new data points does not match the number of columns in the training data.&quot;)
scalar nrows_X = rows(new_data)
scalar ncols_X = cols(new_data)
matrix new_data = prepare_features(new_data, self.stdize_features)
matrix new_data |= self.features
matrix distances = compute_distances(new_data, self.distance_type)
# Remove the initial nrows_X columns which are the distances to the test data
# Keep only the initial nrows_X rows which are the distances for the nrows_X testdata records
distances = distances[1:nrows_X,-seq(1,nrows_X)]
assert(rows(distances) ==  nrows_X)
assert(cols(distances) == rows(self.features))

# Compute the indices for each training record which are the closest to the test record
matrix indices_of_closest = indices_of_closest(distances, self.n_neighbors)

# Compute the predictions for the target using data on the closest neighbors
if self.type == &quot;regression&quot;
  matrix prediction = compute_yhat_regression(self.target, indices_of_closest)
else
  matrix prediction = compute_yhat_classification(self.target, indices_of_closest, self.class_prediction)
endif

return prediction
</code>
</gretl-function>
<gretl-function name="knn_scores" type="matrix">
 <params count="3">
  <param name="actual" type="numeric" const="true"/>
  <param name="pred" type="numeric" const="true"/>
  <param name="self" type="bundle" const="true"/>
 </params>
<code>/*
Calculates the evaluation statistics for the k-nearest neighbors (kNN) algorithm.

Parameters:
actual (numeric): The actual target variable values. Can be a series or a column vector.
pred (numeric): The predicted target variable values. Can be a series or a column vector.
self (bundle): A bundle containing additional information about the algorithm.

Returns:
fcstats (matrix): The forecast statistics matrix.

Notes:
- The target variable must be either a series or a column vector.
- The prediction variable must be either a series or a column vector.
- The bundle 'self' must contain the following properties:
- type (string): The type of the algorithm ('regression' or 'classification').
- class_prediction (string): The method used for class prediction in case of classification ('majority' or 'probability').
*/

if typename(actual) == &quot;list&quot; || typename(pred) == &quot;list&quot;
  errorif(TRUE, &quot;The actual and prediction variable must be either a series or a column vector.&quot;)
endif

if typename(actual) == &quot;series&quot;
  matrix target = {actual}
else
  matrix target = actual
endif

if typename(pred) == &quot;series&quot;
  matrix prediction = {pred}
else
  matrix prediction = pred
endif

if self.type == &quot;regression&quot;
  matrix fcstats = fcstats(target, prediction)
elif self.type == &quot;classification&quot;
  if self.class_prediction == &quot;majority&quot;
    matrix fcstats = fcstats_majority(target, prediction)
  elif self.class_prediction == &quot;probability&quot;
    matrix fcstats = fcstats_probability(target, prediction)
  endif
else
  errorif(TRUE, &quot;unknown estimation class&quot;)
endif

return fcstats
</code>
</gretl-function>
<gretl-function name="knn_summary" type="void">
 <params count="1">
  <param name="self" type="bundle" const="true"/>
 </params>
<code>/* Print a summary of the KNN model. */

printf &quot;\nK-Nearest Neighbors Model Summary\n&quot;
printf &quot;----------------------------------\n&quot;
printf &quot;Type:                       %s\n&quot;, self.type
if self.splitters == &quot;none&quot;
  if typename(self.n_neighbors) == &quot;scalar&quot;
    printf &quot;Number of neighbors:        %d\n&quot;, self.n_neighbors
  else
    printf &quot;Number of neighbors:        %d to %d\n&quot;, min(self.n_neighbors), max(self.n_neighbors)
  endif
else
  printf &quot;Splitter:                   %s\n&quot;, self.splitters
  printf &quot;Optimal no. of neighbors:   %d\n&quot;, self.optimal_k
  printf &quot;Best mean score ('%s'):   %.4f\n&quot;, self.metric, self.optimal_score
endif
printf &quot;Distance type:              %s\n&quot;, self.distance_type
printf &quot;Standardize features:       %s\n&quot;, self.stdize_features ? &quot;Yes&quot; : &quot;No&quot;
printf &quot;Target variable:            %s\n&quot;, self.depvar
printf &quot;Feature variables:          %s\n&quot;, flatten(self.parnames, &quot;, &quot;)
printf &quot;Number of observations:     %d\n&quot;, self.nobs
printf &quot;Sample period:              %d to %d\n&quot;, self.sample_t1, self.sample_t2
if inbundle(self, &quot;rsq&quot;) &amp;&amp; typename(self.rsq) == &quot;scalar&quot;
  printf &quot;R-squared:               %.4f\n&quot;, self.rsq
endif
if inbundle(self, &quot;ess&quot;) &amp;&amp; typename(self.ess) == &quot;scalar&quot;
  printf &quot;Sum of squares:          %.4f\n&quot;, self.ess
endif
printf &quot;----------------------------------\n&quot;
</code>
</gretl-function>
<gretl-function name="knn_plot_score" type="void">
 <params count="2">
  <param name="self" type="bundle" const="true"/>
  <param name="filename" type="string" optional="true" const="true"/>
 </params>
<code>/*
* Plots the optimal score for different values of k in k-nearest neighbors algorithm.
*
* Parameters:
*    self: bundle
*        The bundle containing the mean scores and other information.
*    filename: string (optional)
*        The name of the output file. If not provided, the plot will be displayed.
*
* Notes:
*    - The function requires cross-validation results in the bundle.
*    - The plot will show the optimal score for each value of k.
*    - The x-axis represents the number of neighbors (k), and the y-axis represents the mean score.
*    - The title of the plot includes the optimal score, the optimal k value, and the cross-validation method used.
*    - The plot will be displayed or saved to a file based on the provided filename.
*/

if inbundle(self, &quot;mean_scores&quot;) &amp;&amp; nelem(self.n_neighbors) &gt; 1
  string metric = toupper(self.scoring_regression)
  matrix ms = self.mean_scores
  scalar pos = instrings(cnameget(ms), metric)
  string fname = exists(filename) ? filename : &quot;display&quot;
  string title = sprintf(&quot;Optimal score of %.4f for k = %d, cv: %s&quot;, self.optimal_score, self.optimal_k, self.splitters)

  gnuplot pos --matrix=ms --with-lines --time-series --output=&quot;@fname&quot; {set grid; set xlabel &quot;k neighbors&quot; font &quot;,13&quot;; set ylabel &quot;mean of @metric&quot; font &quot;,13&quot;; set title &quot;@title&quot; font &quot;,13&quot;; set linetype 1 lw 2 lc rgb 'black'; }
else
  print &quot;ERROR: Plot requires cross-validation results and/or multiple neighbors to evaluate.&quot;
endif
</code>
</gretl-function>
<gretl-function name="set_bundle_and_get_defaults" type="bundle" private="1">
 <params count="2">
  <param name="xlist" type="list" const="true"/>
  <param name="opts" type="bundle" optional="true"/>
 </params>
<code>/* Compile self bundle by merging eventual information
from 'opts' bundle. */

if !exists(opts)
  bundle opts = defbundle()
endif

bundle self = default_values()
self = opts + self          # override defaults

return self
</code>
</gretl-function>
<gretl-function name="default_values" type="bundle" private="1">
<code>/*
* This function creates a bundle with default values for the k-nearest neighbors
* (knn) algorithm.
*
* Returns:
*   A bundle containing the default values for the knn algorithm.
*/

bundle self = defbundle()
scalar self.stdize_features = TRUE
string self.distance_type = &quot;euclidean&quot;
string self.class_prediction = &quot;majority&quot;
string self.splitters = &quot;none&quot;  # &quot;stratified_kfold&quot;, &quot;tscv&quot;, &quot;loo&quot;
scalar self.kfold_nsplits = 5   # number of folds for kfold-splitters
scalar self.win_size = 10   # number of folds for timeseries-splitters
string self.scoring_regression = &quot;rmse&quot;
string self.scoring_classification = &quot;FSC&quot;  # FSC=F1-Score
scalar self.nobs = $nobs
scalar self.sample_t1 = $t1
scalar self.sample_t2 = $t2

return self
</code>
</gretl-function>
<gretl-function name="compute_distances" type="matrix" private="1">
 <params count="2">
  <param name="features" type="matrix" const="true"/>
  <param name="distance_type" type="string" const="true"/>
 </params>
<code>/*
Calculates the pairwise distances between the given features.

Parameters:
features: A matrix containing the features in columns and records in rows (=T).
distance_type: string name of distance type

Returns:
D: T by T matrix containing the pairwise distances between the features.
The rows refer to the i-th feature and the columns to the distances
to the j-th feature. The diagonal contains the distances to itself is
zero by definition.
*/
matrix out = distance(features, distance_type)
matrix D = unvech(out, 0)

return D
</code>
</gretl-function>
<gretl-function name="get_folds_label" type="strings" private="1">
 <params count="1">
  <param name="nfolds" type="int" const="true"/>
 </params>
<code>/*
* Generates an array of strings representing the labels for each fold in a cross-validation process.
*
* Parameters:
*   nfolds - The number of folds in the cross-validation process.
*
* Returns:
*   An array of strings, where each string represents the label for a fold.
*/

strings S = array(nfolds)
loop i=1..nfolds
  S[i] = sprintf(&quot;fold=%d&quot;, i)
endloop

return S
</code>
</gretl-function>
<gretl-function name="get_neighbors_label" type="strings" private="1">
 <params count="1">
  <param name="n_neighbors" type="int" const="true"/>
 </params>
<code>/*
* This function generates an array of strings representing the labels for the nearest neighbors.
*
* Parameters:
*   - n_neighbors: The number of nearest neighbors.
*
* Returns:
*   - An array of strings representing the labels for the nearest neighbors.
*/

strings S = array(n_neighbors)
loop i=1..n_neighbors
  S[i] = sprintf(&quot;k=%d&quot;, i)
endloop

return S
</code>
</gretl-function>
<gretl-function name="get_scores_by_neighbor" type="matrix" private="1">
 <params count="1">
  <param name="Scores" type="matrices" const="true"/>
 </params>
<code>/*
* This function calculates the mean score for each neighbor in a set of scores.
*
* Parameters:
* - Scores: A set of matrices containing scores for each neighbor. Each column refers
*  to some metric and each row to some fold on which the model was trained on.
*
* Returns:
* - mean_score_by_neighbor: A matrix containing the mean score for each neighbor.
*/

scalar ncols = cols(Scores[1])
scalar npages = nelem(Scores)
strings clabels = cnameget(Scores[1]) # retrieve metric names
matrix mean_score_by_neighbor = mshape(NA, npages, ncols)

loop i=1..ncols
  matrix mean_score_by_neighbor[,i] = meanc(drill(Scores, , i))'
endloop

cnameset(mean_score_by_neighbor, clabels)
rnameset(mean_score_by_neighbor, get_neighbors_label(npages))

return mean_score_by_neighbor
</code>
</gretl-function>
<gretl-function name="knn_compute" type="void" private="1">
 <params count="4">
  <param name="target" type="matrix" const="true"/>
  <param name="features" type="matrix" const="true"/>
  <param name="self" type="bundleref"/>
  <param name="compute_yhat" type="bool" default="1" const="true"/>
 </params>
<code>/*
* This function computes the k-nearest neighbors (KNN) algorithm.
*
* Parameters:
* - target: The matrix of target variables.
* - features: The matrix of features.
* - self: A bundle containing additional parameters and variables.
* - compute_yhat: A boolean indicating whether to compute the predicted values.
*
* Returns:
* This function does not return any value.
*
* Description:
* The function computes the KNN algorithm by finding the k nearest neighbors for each observation in the features matrix.
* It then computes the predicted values for the target variables based on the type of estimation (regression or classification).
* The computed values are stored in the self.yhat matrix if compute_yhat is set to TRUE.
*/

matrix self.target = target
matrix self.features = prepare_features(features, self.stdize_features)
matrix distances = compute_distances(self.features, self.distance_type)
# Compute the indices for each observation which are the closest to it
matrix indices_of_closest = indices_of_closest(distances, self.n_neighbors)

if compute_yhat == TRUE
  # Compute the predictions for the target using data on the closest neighbors
  if self.type == &quot;regression&quot;
    matrix self.yhat = compute_yhat_regression(target, indices_of_closest)
  elif self.type == &quot;classification&quot;
    matrix self.yhat = compute_yhat_classification(target, indices_of_closest, self.class_prediction)
  else
    errorif(TRUE, &quot;unknown estimation class&quot;)
  endif
endif
</code>
</gretl-function>
<gretl-function name="knnCV" type="void" private="1">
 <params count="3">
  <param name="target" type="matrix" const="true"/>
  <param name="features" type="matrix" const="true"/>
  <param name="self" type="bundleref"/>
 </params>
<code>/*
* This function performs k-fold cross-validation for the k-nearest neighbors (k-NN) algorithm.
* It takes a target matrix and a features matrix as input, along with a bundle containing
* various parameters for the k-NN algorithm. The function splits the data into training and
* validation sets using a specified cross-validation method, and then estimates the k-NN model
* for each training set. Finally, it computes the scores for each validation set and returns
* the results as a matrix.
*
* Parameters:
*    target: The target matrix containing the dependent variable.
*    features: The features matrix containing the independent variables.
*    self: A bundle containing the following parameters for the k-NN algorithm:
*        - kfold_nsplits: The number of folds for cross-validation.
*        - win_size: The window size for the time-series cross-validation method.
*        - splitters: The type of cross-validation method to use.
*        - stdize_features: Whether to standardize the features before fitting the model.
*        - type: The type of k-NN algorithm to use (e.g., classification or regression).
*        - class_prediction: The type of class prediction to use for classification.
*        - n_neighbors: The number of nearest neighbors to consider.
*        - distance_type: The type of distance metric to use.
*
* Returns:
*    The scores matrix, which contains the evaluation scores for each validation set.
*/

# Preparations for CvDataSplitter
bundle DataCV = null
matrix DataCV.X = target ~ features
matrix DataCV.index = seq(1, rows(DataCV.X))'
scalar DataCV.n_folds = self.kfold_nsplits
scalar DataCV.win_size = self.win_size
string DataCV.cv_type = self.splitters
# Retrive the training and validation data
CvDataSplitter(&amp;DataCV)
scalar n_trainsets = nelem(DataCV.X_train)

# Estimate the model for each training set
matrix Scores = {}

loop tset=1..n_trainsets
  bundle model = _(stdize_features = self.stdize_features, type = self.type, class_prediction = self.class_prediction, n_neighbors = self.n_neighbors, distance_type = self.distance_type)
  matrix y_train = DataCV.X_train[tset][,2]  # 1st column refers to index
  matrix X_train = DataCV.X_train[tset][,3:]
  matrix y_test = DataCV.X_test[tset][,2] # 1st column refers to index
  matrix X_test = DataCV.X_test[tset][,3:]
  assert(rows(y_train) == rows(X_train))
  assert(rows(y_test) == rows(X_test))

  # set computing in-sample predictions to FALSE as not needed here
  knn_compute(y_train, X_train, &amp;model, FALSE)
  matrix forecast = knn_predict(model, X_test)  # outforecast using the validation set
  matrix Scores ~= knn_scores(y_test, forecast, model)

  if tset == 1
    strings metric_labels = rnameget(Scores)
  endif
endloop

self.Scores = Scores'
rnameset(self.Scores, get_folds_label(tset))
cnameset(self.Scores, metric_labels)
</code>
</gretl-function>
<gretl-function name="get_optimal_score_and_k" type="bundle" private="1">
 <params count="2">
  <param name="mean_scores" type="matrix" const="true"/>
  <param name="metric" type="string" const="true"/>
 </params>
<code>/* Based on cross-validated mean scores, retrieve the optimal number of
neighbors based on some metric. */

catch scalar position_criteria = instrings(cnameget(mean_scores), toupper(metric))
errorif($error, &quot;Unknown metric name passed.&quot;)

bundle B = empty
scalar B.optimal_k = iminc(mean_scores[,position_criteria], TRUE)
scalar B.optimal_score = NA

if !ok(B.optimal_k)
  printf &quot;ERROR: Cannot compute optimal no. of neighbors for metric '%s'.\n&quot;, metric
  print &quot;Please select another metric instead.&quot;
  return B
endif

scalar B.optimal_score = mean_scores[B.optimal_k, position_criteria]
print B.optimal_score

return B
</code>
</gretl-function>
<gretl-function name="fcstats_majority" type="matrix" private="1">
 <params count="2">
  <param name="actual" type="matrix" const="true"/>
  <param name="prediction" type="matrix" const="true"/>
 </params>
<code>/**
* Compute evaluation statistics for binary integer outcomes using the majority rule.
*
* actual: The matrix of true actual values.
* prediction: The matrix of predicted values.
* return: The matrix of evaluation statistics, including false rate, hit rate, and Kuipers score.
*/

/* TODO: Test first whether metrics are correctly computed
bundle B = _(yup = actual, fcup = prediction)
doKS(&amp;B)
matrix fcstats = B.KSfalse | B.KShit | B.KSstat
rnameset(fcstats, defarray(&quot;false-rate&quot;, &quot;hit-rate&quot;, &quot;Kuipers-score&quot;))
*/

# TODO: Results do not coincide. Check which function returns wrong values.
matrix scores = scores2x2(actual ~ prediction, FALSE)

return scores
</code>
</gretl-function>
<gretl-function name="fcstats_probability" type="matrix" private="1">
 <params count="2">
  <param name="actual" type="matrix" const="true"/>
  <param name="prediction" type="matrix" const="true"/>
 </params>
<code>/*
* This function computes evaluation statistics for binary classification models.
*
* Parameters:
* - actual: A matrix containing the actual class labels.
* - prediction: A matrix containing the predicted probabilities for each class.
*
* Returns:
* - fcstats: A matrix containing the evaluation statistics (computed by the &quot;FEP&quot; package).
*   - If the number of unique values in 'actual' is 2, the column vector
*     contains the following statistics:
*     - Quadratic probability score (QPS)
*     - Logarithmic probability score (LPS)
*   - If the number of unique values in 'actual' is not 2, a warning message is printed and an empty matrix is returned.
*/

if nelem(values(actual)) == 2
  matrix fcstats = probscore(actual, prediction)
  strings label = cnameget(fcstats)
  fcstats = fcstats'
  rnameset(fcstats, label)
else
  printf &quot;\nWARNING: No support for statistics for more than three class outcomes.\n\n&quot;
  matrix fcstats = {}
endif

return fcstats
</code>
</gretl-function>
<gretl-function name="prepare_features" type="matrix" private="1">
 <params count="2">
  <param name="features" type="matrix"/>
  <param name="stdize_features" type="bool" const="true"/>
 </params>
<code>/*
* This function prepares the features matrix for k-nearest neighbors (KNN) algorithm.
*
* Parameters:
*   - features: A matrix containing the features.
*   - stdize_features: A boolean indicating whether to standardize the features.
*
* Returns:
*   - A matrix containing the prepared features.
*/
if stdize_features &amp;&amp; rows(features) &gt; 1
  matrix features = stdize(features, 0, TRUE)
endif

return features
</code>
</gretl-function>
<gretl-function name="get_rsq" type="scalar" private="1">
 <params count="2">
  <param name="target" type="matrix" const="true"/>
  <param name="yhat" type="matrix" const="true"/>
 </params>
<code>/*
* Calculates the R-squared value between the target and predicted values.
*
* target: The matrix of target values.
* yhat: The matrix of predicted values.
* return: The R-squared value.
*/
scalar rsq = mcorr(target~yhat)[1,2]^2
return rsq
</code>
</gretl-function>
<gretl-function name="compute_yhat_regression" type="matrix" private="1">
 <params count="2">
  <param name="target" type="matrix" const="true"/>
  <param name="indices_of_closest" type="matrix" const="true"/>
 </params>
<code>/*
* Compute the predicted values (yhat) based on the target matrix and
* the indices of the closest neighbors for regressions.
*
* target: The target matrix containing the true values.
* indices_of_closest: The matrix containing the indices of the closest neighbors.
*
* return: The matrix containing the predicted values (yhat).
*/

matrix yhat = target_means(target, indices_of_closest)
return yhat
</code>
</gretl-function>
<gretl-function name="compute_yhat_classification" type="matrix" private="1">
 <params count="3">
  <param name="target" type="matrix" const="true"/>
  <param name="indices_of_closest" type="matrix" const="true"/>
  <param name="class_prediction" type="string" const="true"/>
 </params>
<code>/*
* Compute the predicted values (yhat) based on the target matrix and
* the indices of the closest neighbors for classification.
*
* target: The target matrix containing the true values.
* indices_of_closest: The matrix containing the indices of the closest neighbors.
*
* return: The matrix containing the predicted values (yhat).
*/

matrix yhat = target_class(target, indices_of_closest, class_prediction)

return yhat
</code>
</gretl-function>
<gretl-function name="target_means" type="matrix" private="1">
 <params count="2">
  <param name="target" type="matrix" const="true"/>
  <param name="indices_of_closest" type="matrix" const="true"/>
 </params>
<code>/*
* Calculates the mean values of the target variable from the training data
* for each observation of the closest neighbors (measured relative to the test data
* record).
*
* target: The matrix containing the target variable values from the training data.
* indices_of_closest: The matrix containing the indices of the closest
neighbors for each observation.

* return: The matrix of mean values of the target variable for each observation.
*/

scalar N = cols(indices_of_closest)
matrix yhat = mshape(NA, N, 1)

loop i=1..N
  matrix idx = indices_of_closest[,i]
  matrix yhat[i] = mean(target[idx])
endloop

return yhat
</code>
</gretl-function>
<gretl-function name="target_class" type="matrix" private="1">
 <params count="3">
  <param name="target" type="matrix" const="true"/>
  <param name="indices_of_closest" type="matrix" const="true"/>
  <param name="class_prediction" type="string" const="true"/>
 </params>
<code>/*
* This function calculates either the mode or conditional probability of the target
* variable for each observation based on the indices of the closest neighbors.
*
* Parameters:
* - target: A matrix containing the target variable values for all observations.
* - indices_of_closest: A matrix containing the indices of the closest
*   neighbors for each observation.
* - class_prediction: Type of prediction; either `majority` or `probability`.
*
* Returns:
* - yhat: A matrix containing either the mode or conditional probability of
*   the target variable for each observation.
*/

scalar N = cols(indices_of_closest)
matrix yhat = mshape(NA, N, 1)
scalar information = class_prediction == &quot;majority&quot; ? 1 : 2

loop i = 1..N
  matrix idx = indices_of_closest[,i]
  matrix value = onemode(target[idx])[information]

  if ok(value)
    matrix yhat[i] = value
  else
    printf &quot;WARNING: Could not compute prediction for observation %d. Ignore.\n&quot;, $i
  endif
endloop

return yhat
</code>
</gretl-function>
<gretl-function name="indices_of_closest" type="matrix" private="1">
 <params count="2">
  <param name="distances" type="matrix" const="true"/>
  <param name="n_neighbors" type="int" const="true"/>
 </params>
<code>/*
This function calculates the indices of the closest neighbors based on the given distances.

Parameters:
- distances: matrix, the distances between observations. The rows refer to
each testdata record. The columns refer to the distance of each testdata
observation to each point of the training set
- n_neighbors: int, the number of closest neighbors to consider.

Returns:
- idx: matrix, the indices of the closest neighbors (in rows) for each
testdata record (in columns). The rows equal the number of neighbors.
*/

scalar N = rows(distances)
matrix indices_of_closest = mshape(NA, n_neighbors, N)
matrix idx = seq(1, cols(distances))'

# TODO: Check whether this can be improved in terms of computation
# TODO: In principle this may be parallelized but not in this version

loop i = 1..N
  matrix row = distances[i,]' ~ idx
  # sort by distance in ascending order
  matrix sorted = msortby(row, 1)[-i,]  # remove reference observation
  matrix indices_of_closest[,i] = sorted[1:n_neighbors, 2]
endloop

return indices_of_closest
</code>
</gretl-function>
<sample-script>
clear
set verbose off
include knn.gfn
/*
open credscore.gdt --quiet

# Select an example to run
EXAMPLE = 2
# Parameter
scalar N_NEIGHBORS = 5  # no. of neighbors

if EXAMPLE == 1  # regression
    list x = Age MDR OwnRent
    series y = Income

elif EXAMPLE == 2  # classification with majority voting
    list x = Age Income
    series y = Acc
    setinfo y --discrete

elif EXAMPLE == 3  # classification with probability predictions
    list x = Age Income
    series y = Acc
    setinfo y --discrete
    bundle Params = _(class_prediction = &quot;probability&quot;,
                      distance_type = &quot;manhatten&quot;)
endif

# Remove missing values and define training set
smpl y x --no-missing --permanent
genr index
series trainset = (index &lt;= 50) ? TRUE : FALSE

# Activate training set
smpl trainset == TRUE --restrict

# Euclidean distance (default)
# ============================
if exists(Params)
    bundle Model = knn_fit(y, x, N_NEIGHBORS, Params)
else
    bundle Model = knn_fit(y, x, N_NEIGHBORS)
endif

knn_summary(Model)  # Print summary of estimation
series yhat = Model.yhat
series uhat = Model.uhat

if EXAMPLE == 1
    series diagonal = y
    gnuplot yhat diagonal y --with-lines=diagonal --output=display \
      {set yrange[0:10]; set xrange[0:10];}

    freq uhat --normal --plot=display

elif EXAMPLE == 2
    if Model.class_prediction == &quot;majority&quot;
        gnuplot Age Avgexp yhat --dummy --output=display --fit=none
    endif
endif

# In-sample scores
print &quot;In-sample scores&quot;
print knn_scores(y, yhat, Model)

# Activate testset and predict out-of-sample
smpl trainset == FALSE --restrict --replace
series prediction = knn_predict(Model, x)
# Out-of-sample scores
print &quot;Out-of-sample scores&quot;
print knn_scores(y, prediction, Model)
#print y prediction -o --range=:15
*/


#####################################
### Regression using cross-validation
# Run k-fold with 5 folds sampling
#####################################
open credscore.gdt --quiet
# Set parameters
matrix NEIGHBORS = seq(1,4)  # sequence of no. of neighbors to evaluate
scalar KFOLD_NSPLITS = 5  # only relevant for splitters = &quot;kfold&quot;
string SPLITTERS = &quot;kfold&quot;  # either &quot;kfold&quot;, &quot;loo&quot;
string METRIC = &quot;mae&quot;   # metric to optimize
bundle Params = _(splitters = SPLITTERS, kfold_nsplits = KFOLD_NSPLITS,
                  scoring_regression = METRIC)
# Select features and target
list x = Age Selfempl OwnRent
series y = Income
# Fit the model
bundle Model = knn_fit(y, x, NEIGHBORS, Params)
print Model
knn_summary(Model)
knn_plot_score(Model)
</sample-script>
</gretl-function-package>
</gretl-functions>
