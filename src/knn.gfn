<?xml version="1.0" encoding="UTF-8"?>
<gretl-functions>
<gretl-function-package name="knn" minver="2024a">
<author email="atecon@posteo.de">Artur Tarassow</author>
<version>0.1</version>
<date>2024-06-20</date>
<description>KNN regression and classification estimator</description>
<tags>C13 C52</tags>
<help filename="knn_help.md">
# KNN Package Documentation

The KNN (K-Nearest Neighbors) package is a powerful tool for performing KNN classification and regression tasks. It provides a set of functions that allow you to fit a model, make predictions, get scores, summarize the model, and plot the scores.

Please report any issues or suggestions on the Gretl mailing list or GitHub page: https://github.com/atecon/knn .


# Public Functions

### `knn_fit(train_data, train_labels, n_neighbors, opts[null])`

This function fits the KNN model to the training data.

*Parameters:*

- `y`: *series*, The training data to fit the model on.
- `xlist`: *list*, The features to use for the KNN algorithm.
- `n_neighbors`: *int* or *matrix*, The number of neighbors to use for the KNN algorithm. If an integer is provided, a single KNN model is fitted. If a matrix is provided, multiple KNN models are fitted with different numbers of neighbors.
- `opts[null]`: *bundle*, Optionally, a bundle of options to pass to the KNN algorithm.

The `opts` bundle can contain the following options:

- `distance_type`: *string*, The distance metric to use for the KNN algorithm. Default is &quot;euclidean&quot;. Possible values are the ones supported by Gretl's built-in function `distance()` (see `help distance`).
- `class_prediction`: *string*, The method to use for predicting classes in a classification task. Default is &quot;majority&quot;. &lt;!-- Possible values are--&gt;

  + &quot;majority&quot;
&lt;!--  + &quot;probability&quot; (only for binary classification, currently). --&gt;

  Majority returns the most common class among the neighbors.&lt;!--, while probability returns the proportion of neighbors that belong to the class most common among the neighbors.--&gt;

- `scoring_regression`: *string*, The method to use for scoring the model in a regression task. Default is &quot;rmse&quot;. Possible values are:

  + &quot;me&quot;

  + &quot;rmse&quot;

  + &quot;mae&quot;

  + &quot;mape&quot;

  + and others supported by Gretl's built-in function `fcstats()` (see `help fcstats`).

- `scoring_classification`: *string*, The method to use for scoring the model in a classification task. Default is &quot;PRC&quot; referring to precision which is the number of hits over the sum of hits plus false, h/(h+f).
&lt;!-- Default is &quot;FSC&quot; referring to the F1-score which balances recall and precision equally and reduces to the simpler equation 2TP/(2TP + FP + FN). --&gt;
  Alternatives are:


  + &quot;POD&quot;: Prob. of detection

  + &quot;POFD&quot;: Prob. of false detection

  + &quot;HR&quot;: Hit rate

  + &quot;FAR&quot;: False alaram rate

  + &quot;CSI&quot;: Critical success index

  + &quot;OR&quot;: Odds ratio

  + &quot;BIAS&quot;: Bias score

  + &quot;TSS&quot;: Hanssen-Kuipers score (POD - POFD)

  + &quot;HSS&quot;: Heidke skill score

  + &quot;ETS&quot;: Equitable threat score

  + &quot;PRC&quot;: Precision

  + &quot;FSC&quot;: F-Score

- `splitters`: *string*, The method to use for splitting the data into training and test sets. Default is &quot;none&quot; implying that the data is not split and no cross-validation is performed. Possible values are:

  + &quot;none&quot;: no cross-validation is performed.

  + &quot;kfold&quot;: perform k-fold cross-validation with the number of folds specified by the `kfold_nsplits` parameter (default: 5).

  + &quot;loo&quot;: perform leave-one-out cross-validation; only for regression.

  + &quot;recwin&quot;: perform recursive window cross-validation with the window size specified by the &quot;win_size&quot; parameter (default: 10).

  + &quot;rolwin&quot;: perform rolling window cross-validation with the window size specified by the &quot;win_size&quot; parameter (default: 10).

- `stdize_features`: *bool*, Whether to standardize the features before fitting the model. Default is &quot;TRUE&quot;.


**Returns:**

A fitted KNN model object stored in a `bundle`. The bundle includes the following elements:

- `depvar`: *string*, The dependent variable used for fitting the model.
- `features`: *matrix*, The features used for fitting the model; if `stdize_features` is set to `TRUE`, the features are standardized.
- `mean_scores`: *matrix*, The mean scores achieved by the model on the validation data for each number of neighbors (only if cross-validation is performed). Rows represent the number of neighbors used, and columns represent the scoring metrics.
- `n_training_sets`: *int*, The number of training sets used for cross-validation (only if cross-validation is performed).
- `nobs`: *int*, Number of observations in the training and validation data.
- `optimal_k`: *int*, The optimal number of neighbors selected by the cross-validation procedure (only if cross-validation is performed).
- `optimal_score`: *scalar*, The optimal score achieved by the model on the validation data (only if cross-validation is performed).
- `parnames`: *string array*, The names of the features used for fitting the model.
- `sample_t1`: *int*, The index of the first observation in the training set.
- `sample_t2`: *int*, The index of the last observation in the training set.
- `Scores`: *matrices*, Array of matrices containing the scores achieved by the model on the validation data. Each page refers to a different number of neighbors (as specified by `n_neighbors`) evaluated. Rows represent the k-fold splits, and columns represent the scoring metrics.
- `type`: *string*, The type of the model (classification or regression).
- `uhat`: *matrix*, The residuals of the model (only in case of no cross-validation); rows: no. of observations, columns: no. of neighbors evaluated.
- `yhat`: *matrix*, The fitted values of the model (only in case of no cross-validation); rows: no. of observations, columns: no. of neighbors evaluated.



### `knn_predict(model, X)`

This function uses a fitted KNN model to make predictions on the test data. The function requires that you either have requested to train a model with a single number of neighbors or have selected the optimal number of neighbors using cross-validation before, as the model object must contain the optimal number of neighbors. Otherwise, the function will return an error.

*Parameters:*

- `model`: *bundle*, The fitted KNN model object.
- `X`: *numeric*, A list or matrix of the test data to make predictions on.

**Returns:**

- A matrix of predictions.


### `knn_scores(actual, pred, model)`

This function calculates the scores of the model prediction.

*Parameters:*

- `actual`: *series* or *matrix*, The actual data.
- `pred`: *series* or *matrix*, The predicted data.
- `model`: The fitted KNN model.

**Returns:**

A *matrix* holding various accuracy scores.


### `knn_summary(model)`

This function provides a summary of the KNN model.

*Parameters:*

- `model`: *bundle*, The fitted KNN model.

**Returns:**

- A summary of the model.


### `knn_plot_score(model, filename[null])`

This function generates a plot showing the mean performance (across all cross-validation iterations) of the KNN model as a function of the number of neighbors. Only available if cross-validation is performed. The plot shows the mean of the selected metric (e.g., RMSE, MAE, etc.) across all cross-validation iterations for each number of neighbors evaluated

*Parameters:*

- `model`: The fitted KNN model.
- `filename`: *string*, The name of the file to save the plot to. If not provided, the plot is displayed in the Gretl GUI.

**Returns:**

- A plot showing the model's performance.


### `knn_plot_cvscores(model, filename[null])`

This function generates a plot showing the distribution of the performances across folds as a function of the number of neighbors. Only available if cross-validation is performed. The plot shows a boxplot of the selected metric (e.g., RMSE, MAE, etc.) across folds for each number of neighbors evaluated.

*Parameters:*

- `model`: The fitted KNN model.
- `filename`: *string*, The name of the file to save the plot to. If not provided, the plot is displayed in the Gretl GUI.

**Returns:**

- A plot showing the model's performance.

# Change Log

- v0.1 (June 2024): Initial release.
</help>
<depends count="3">
extra FEP CvDataSplitter </depends>
<gretl-function name="knn_fit" type="bundle">
 <params count="4">
  <param name="y" type="series" const="true">
<description>Target</description>
  </param>
  <param name="xlist" type="list" const="true">
<description>Features</description>
  </param>
  <param name="n_neighbors" type="numeric" const="true">
<description>No. of neighbors</description>
  </param>
  <param name="opts" type="bundle" optional="true">
<description>Parameter bundle</description>
  </param>
 </params>
<code>/* Function for computing KNN: run the setup and train the model. */

list L = y xlist
errorif(sum(missing(L)), &quot;Some  have missing values. Please drop these first.&quot;)
errorif($nobs &lt;= n_neighbors, &quot;Number of rows must exceed number of neighbors.&quot;)
errorif(typename(n_neighbors) != &quot;scalar&quot; &amp;&amp; typename(n_neighbors) != &quot;matrix&quot;, &quot;Parameter 'n_neighbors' must bei either of type scalar or matrix.&quot;)
errorif(isconst(y) == TRUE, &quot;Dependent variable is constant. Abort.&quot;)

if !exists(opts)
  bundle opts = defbundle()
endif

bundle self = set_bundle_and_get_defaults(xlist, opts)
self.distance_type = tolower(self.distance_type)
string self.depvar = argname(y)
strings self.parnames = varnames(xlist)
string self.type = getinfo(y).discrete == TRUE ? &quot;classification&quot; : &quot;regression&quot;
string self.metric = get_metric_name(self)

errorif(self.class_prediction == &quot;probability&quot;, &quot;Computing probabilities not yet supportted.&quot;)
check_metric_probclass(self.type, self.metric, self.class_prediction)

errorif_classification_loo(self.type, self.splitters)
errorif_multiclass_prob(y, self.class_prediction, self.type)

series self.target = y
matrix my = {y}
matrix mX = {xlist}
scalar self.n_features = cols(mX)
scalar self.nobs_features = rows(mX)
matrix neighbor_values = values(vec(n_neighbors))
scalar n_neighbor_values = nelem(neighbor_values)

if self.splitters == &quot;none&quot;
  matrix self.yhat = mshape(NA, self.nobs_features, n_neighbor_values)
  matrix self.uhat = mshape(NA, self.nobs_features, n_neighbor_values)
  if self.type == &quot;regression&quot;
    matrix self.ess = mshape(NA, n_neighbor_values, 1)
    matrix self.rsq = mshape(NA, n_neighbor_values, 1)
    strings rlabel_rsq_ess = array(n_neighbor_values)
    string clabel_rsq = &quot;R^2&quot;
    string clabel_ess = &quot;ess&quot;
  endif
else
  # each page refers to different number of folds
  matrices self.Scores = array(n_neighbor_values)
endif

matrix self.features = prepare_features(mX, self.stdize_features)

loop nb=1..n_neighbor_values
  #printf &quot;\nStart analysis for k = %d neighbors.\n&quot;, neighbor_values[nb]
  scalar self.n_neighbors = neighbor_values[nb]

  if self.splitters == &quot;none&quot;
    # Standardization is done using the whole dataset
    matrix self.yhat[,nb] = knn_compute(self, my, self.features)
    matrix self.uhat[,nb] = my - self.yhat[,nb]

    if self.type == &quot;regression&quot;
      scalar self.rsq[nb] = get_rsq(my, self.yhat[,nb])
      scalar self.ess[nb] = sst(self.uhat[,nb])
      rlabel_rsq_ess[nb] = sprintf(&quot;k=%d&quot;, neighbor_values[nb])
    elif self.type == &quot;classification&quot;
      # TODO: implement support for classification?
    endif
  else
    #matrix self.features = mX
    bundle model = self
    knnCV(my, mX, &amp;model)
    self.Scores[nb] = model.Scores
    if nb == 1
      self.n_training_sets = model.n_training_sets
    endif
  endif
  #printf &quot;\nFinished analysis for k = %d neighbors.\n&quot;, neighbor_values[nb]
endloop

# Overwrite the temporary scalar value by the matrix holding all distinct values evaluated
delete self.n_neighbors
self.n_neighbors = n_neighbors

if inbundle(self, &quot;rsq&quot;)
  cnameset(self.rsq, clabel_rsq)
  rnameset(self.rsq, rlabel_rsq_ess)
endif
if inbundle(self, &quot;ess&quot;)
  cnameset(self.ess, clabel_ess)
  rnameset(self.ess, rlabel_rsq_ess)
endif

if inbundle(self, &quot;Scores&quot;)
  matrix self.mean_scores = get_scores_by_neighbor(self.Scores)
  bundle Btmp = get_optimal_score_and_k(self.mean_scores, self.metric, self.type)
  self.optimal_k = Btmp.optimal_k
  self.optimal_score = Btmp.optimal_score
endif

return self
</code>
</gretl-function>
<gretl-function name="knn_predict" type="matrix">
 <params count="2">
  <param name="self" type="bundle" const="true"/>
  <param name="X" type="numeric" const="true"/>
 </params>
<code>/*
This function performs k-nearest neighbors prediction on new data points.

Parameters:
- self: The bundle containing the training data and model parameters.
- X: The new data points to be predicted.

Returns:
- prediction: The predicted values for the new data points.

Notes:
- The function checks if the number of columns in the new data points matches the number of columns in the training data.
- The function prepares the new data points by standardizing the features if required.
- The function computes the distances between the new data points and the training data.
- The function selects the closest neighbors for each new data point.
- The function computes the predicted values based on the type of problem (regression or classification).
*/

if typename(X) == &quot;series&quot; || typename(X) == &quot;list&quot;
  matrix new_data = {X}
else
  matrix new_data = X
endif

errorif(cols(new_data) != cols(self.features), &quot;The number of columns in the new data points does not match the number of columns in the training data.&quot;)
scalar nrows_X = rows(new_data)
scalar ncols_X = cols(new_data)
matrix new_data = prepare_features(new_data, self.stdize_features)
matrix new_data |= self.features
matrix distances = compute_distances(new_data, self.distance_type)
# Remove the initial nrows_X columns which are the distances to the test data
# Keep only the initial nrows_X rows which are the distances for the nrows_X testdata records
distances = distances[1:nrows_X,-seq(1,nrows_X)]
assert(rows(distances) ==  nrows_X)
assert(cols(distances) == rows(self.features))

# Compute the indices for each training record which are the closest to the test record
if inbundle(self, &quot;optimal_k&quot;)
  matrix indices_of_closest = indices_of_closest(distances, self.optimal_k)
else
  errorif(nelem(self.n_neighbors) &gt; 1, sprintf(&quot;You passed a vector of neighbors to evaluate.\n Please run CV first to estimate the optimal no. of neighbors before doing prediction.&quot;))
  matrix indices_of_closest = indices_of_closest(distances, self.n_neighbors)
endif

# Compute the predictions for the target using data on the closest neighbors
if typename(self.target) == &quot;series&quot;
  matrix target = {self.target}
else
  matrix target = self.target
endif
if self.type == &quot;regression&quot;
  matrix prediction = compute_yhat_regression(target, indices_of_closest)
else
  matrix prediction = compute_yhat_classification(target, indices_of_closest, self.class_prediction)
endif

return prediction
</code>
</gretl-function>
<gretl-function name="knn_scores" type="matrix">
 <params count="3">
  <param name="actual" type="numeric" const="true"/>
  <param name="pred" type="numeric" const="true"/>
  <param name="self" type="bundle" const="true"/>
 </params>
<code>/*
Calculates the evaluation statistics for the k-nearest neighbors (kNN) algorithm.

Parameters:
actual (numeric): The actual target variable values. Can be a series or a column vector.
pred (numeric): The predicted target variable values. Can be a series or a column vector.
self (bundle): A bundle containing additional information about the algorithm.

Returns:
fcstats (matrix): The forecast statistics matrix.

Notes:
- The target variable must be either a series or a column vector.
- The prediction variable must be either a series or a column vector.
- The bundle 'self' must contain the following properties:
- type (string): The type of the algorithm ('regression' or 'classification').
- class_prediction (string): The method used for class prediction in case of classification ('majority' or 'probability').
*/

if typename(actual) == &quot;list&quot; || typename(pred) == &quot;list&quot;
  errorif(TRUE, &quot;The actual and prediction variable must be either a series or a column vector.&quot;)
endif

if typename(actual) == &quot;series&quot;
  matrix target = {actual}
else
  matrix target = actual
endif

if typename(pred) == &quot;series&quot;
  matrix prediction = {pred}
else
  matrix prediction = pred
endif

if self.type == &quot;regression&quot;
  matrix fcstats = fcstats(target, prediction)
elif self.type == &quot;classification&quot;
  if self.class_prediction == &quot;majority&quot;
    matrix fcstats = fcstats_majority(target, prediction)
  elif self.class_prediction == &quot;probability&quot;
    matrix fcstats = fcstats_probability(target, prediction)
  endif
else
  errorif(TRUE, &quot;unknown estimation class&quot;)
endif

return fcstats
</code>
</gretl-function>
<gretl-function name="knn_summary" type="void">
 <params count="1">
  <param name="self" type="bundle" const="true"/>
 </params>
<code>/* Print a summary of the KNN model. */

printf &quot;\nK-Nearest Neighbors Model Summary\n&quot;
printf &quot;----------------------------------\n&quot;
printf &quot;Type:                       %s\n&quot;, self.type
if self.splitters == &quot;none&quot;  # no CV performed
  if typename(self.n_neighbors) == &quot;scalar&quot;
    printf &quot;Number of neighbors:        %d\n&quot;, self.n_neighbors
  else
    printf &quot;Number of neighbors:        %d to %d\n&quot;, min(self.n_neighbors), max(self.n_neighbors)
  endif
else
  printf &quot;Splitter:                   %s\n&quot;, self.splitters
  printf &quot;No. of training sets        %d\n&quot;, self.n_training_sets
  printf &quot;Optimal no. of neighbors:   %d\n&quot;, self.optimal_k
  printf &quot;Score name:                '%s'\n&quot;, self.metric
  printf &quot;Best mean score:            %.4f\n&quot;, self.optimal_score
endif
printf &quot;Distance type:              %s\n&quot;, self.distance_type
printf &quot;Standardize features:       %s\n&quot;, self.stdize_features ? &quot;Yes&quot; : &quot;No&quot;
printf &quot;Target variable:            %s\n&quot;, self.depvar
printf &quot;Feature variables:          %s\n&quot;, flatten(self.parnames, &quot;, &quot;)
printf &quot;Number of observations:     %d\n&quot;, self.nobs
printf &quot;Sample period:              %d to %d\n&quot;, self.sample_t1, self.sample_t2
if inbundle(self, &quot;rsq&quot;) &amp;&amp; typename(self.rsq) == &quot;scalar&quot;
  printf &quot;R-squared:               %.4f\n&quot;, self.rsq
endif
if inbundle(self, &quot;ess&quot;) &amp;&amp; typename(self.ess) == &quot;scalar&quot;
  printf &quot;Sum of squares:          %.4f\n&quot;, self.ess
endif
printf &quot;----------------------------------\n&quot;
</code>
</gretl-function>
<gretl-function name="knn_plot_score" type="void">
 <params count="2">
  <param name="self" type="bundle" const="true"/>
  <param name="filename" type="string" optional="true" const="true"/>
 </params>
<code>/*
* Plots the optimal score for different values of k in k-nearest neighbors algorithm.
*
* Parameters:
*    self: bundle
*        The bundle containing the mean scores and other information.
*    filename: string (optional)
*        The name of the output file. If not provided, the plot will be displayed.
*
* Notes:
*    - The function requires cross-validation results in the bundle.
*    - The plot will show the optimal score for each value of k.
*    - The x-axis represents the number of neighbors (k), and the y-axis represents the mean score.
*    - The title of the plot includes the optimal score, the optimal k value, and the cross-validation method used.
*    - The plot will be displayed or saved to a file based on the provided filename.
*/

if inbundle(self, &quot;mean_scores&quot;) &amp;&amp; nelem(self.n_neighbors) &gt; 1
  if self.type == &quot;regression&quot;
    string metric = toupper(self.scoring_regression)
  elif self.type == &quot;classification&quot;
    string metric = toupper(self.scoring_classification)
  endif
  scalar pos = instrings(cnameget(self.mean_scores), metric)
  matrix toplot = self.mean_scores[,pos] ~ vec(uniq(self.n_neighbors))
  string fname = exists(filename) ? filename : &quot;display&quot;
  string title = sprintf(&quot;Optimal score of %.4f for k = %d, cv: %s&quot;, self.optimal_score, self.optimal_k, self.splitters)

  gnuplot --matrix=toplot --with-lines --fit=none --output=&quot;@fname&quot; {set grid; set xlabel &quot;k neighbors&quot; font &quot;,13&quot;; set ylabel &quot;mean of @metric&quot; font &quot;,13&quot;; set title &quot;@title&quot; font &quot;,13&quot;; set linetype 1 lw 2; }
else
  print &quot;WARNING: Plot requires cross-validation results and/or multiple neighbors to evaluate.&quot;
endif
</code>
</gretl-function>
<gretl-function name="knn_plot_cvscores" type="void">
 <params count="2">
  <param name="self" type="bundle" const="true"/>
  <param name="filename" type="string" optional="true" const="true"/>
 </params>
<code>/*
* Plots the distribution of cross-validated scores for different values of k in k-nearest neighbors algorithm.
*
* Parameters:
*    self: bundle
*        The bundle returned by model fitting.
*    filename: string (optional)
*        The name of the output file. If not provided, the plot will be displayed.
*
* Notes:
*    - The function requires cross-validation results in the bundle.
*    - The plot will show a boxplot of the distribution of the scores across folds for each value of k.
*    - The x-axis represents the number of neighbors (k), and the y-axis represents the score.
*    - The plot will be displayed or saved to a file based on the provided filename.
*/

if inbundle(self, &quot;Scores&quot;)
  if self.type == &quot;regression&quot;
    string metric = toupper(self.scoring_regression)
  elif self.type == &quot;classification&quot;
    string metric = toupper(self.scoring_classification)
  endif
  scalar pos = instrings(cnameget(self.mean_scores), metric)
  # Retrieve cv scores for each no. of neighbors
  matrix scores = drill(self.Scores, null, pos)  # rows=folds, cols: n-neighbors
  cnameset(scores, get_neighbors_label(nelem(self.n_neighbors)))
  string fname = exists(filename) ? filename : &quot;display&quot;
  string title = sprintf(&quot;Optimal k=%d using '%s' (nfolds = %d)&quot;, self.optimal_k, self.splitters, rows(scores))

  if sum(meanc(scores, TRUE) .= 0)  # check for constants in which case the boxplot fails to be drawn
    gnuplot --matrix=scores --time-series --output=&quot;@fname&quot; {set grid; set xlabel &quot;neighbors&quot; font &quot;,13&quot;; set ylabel &quot;Distribution of @metric across folds&quot; font &quot;,13&quot;; set title &quot;@title&quot; font &quot;,13&quot;; set linetype 1 lw 2; }
  else
    boxplot --matrix=scores --output=&quot;@fname&quot; {set grid; set xlabel &quot;neighbors&quot; font &quot;,13&quot;; set ylabel &quot;Distribution of @metric across folds&quot; font &quot;,13&quot;; set title &quot;@title&quot; font &quot;,13&quot;; set linetype 1 lw 2; }
  endif
else
  print &quot;WARNING: Plot requires cross-validation results.&quot;
endif
</code>
</gretl-function>
<gretl-function name="set_bundle_and_get_defaults" type="bundle" private="1">
 <params count="2">
  <param name="xlist" type="list" const="true"/>
  <param name="opts" type="bundle" optional="true"/>
 </params>
<code>/* Compile self bundle by merging eventual information
from 'opts' bundle. */

if !exists(opts)
  bundle opts = defbundle()
endif

bundle self = default_values()
self = opts + self          # override defaults

return self
</code>
</gretl-function>
<gretl-function name="default_values" type="bundle" private="1">
<code>/*
* This function creates a bundle with default values for the k-nearest neighbors
* (knn) algorithm.
*
* Returns:
*   A bundle containing the default values for the knn algorithm.
*/

bundle self = defbundle()
scalar self.stdize_features = TRUE
string self.distance_type = &quot;euclidean&quot;
string self.class_prediction = &quot;majority&quot;
string self.splitters = &quot;none&quot;  # &quot;kfold&quot;, &quot;tscv&quot;, &quot;loo&quot;
scalar self.kfold_nsplits = 5   # number of folds for kfold-splitters
scalar self.win_size = 10   # number of folds for timeseries-splitters
string self.scoring_regression = &quot;rmse&quot;
string self.scoring_classification = &quot;PRC&quot;  # FSC=F1-Score
scalar self.nobs = $nobs
scalar self.sample_t1 = $t1
scalar self.sample_t2 = $t2

return self
</code>
</gretl-function>
<gretl-function name="compute_distances" type="matrix" private="1">
 <params count="2">
  <param name="features" type="matrix" const="true"/>
  <param name="distance_type" type="string" const="true"/>
 </params>
<code>/*
Calculates the pairwise distances between the given features.

Parameters:
features: A matrix containing the features in columns and records in rows (=T).
distance_type: string name of distance type

Returns:
D: T by T matrix containing the pairwise distances between the features.
The rows refer to the i-th feature and the columns to the distances
to the j-th feature. The diagonal contains the distances to itself is
zero by definition.
*/
matrix out = distance(features, distance_type)
matrix D = unvech(out, 0)

return D
</code>
</gretl-function>
<gretl-function name="get_folds_label" type="strings" private="1">
 <params count="1">
  <param name="nfolds" type="int" const="true"/>
 </params>
<code>/*
* Generates an array of strings representing the labels for each fold in a cross-validation process.
*
* Parameters:
*   nfolds - The number of folds in the cross-validation process.
*
* Returns:
*   An array of strings, where each string represents the label for a fold.
*/

strings S = array(nfolds)
loop i=1..nfolds
  S[i] = sprintf(&quot;fold=%d&quot;, i)
endloop

return S
</code>
</gretl-function>
<gretl-function name="get_neighbors_label" type="strings" private="1">
 <params count="1">
  <param name="n_neighbors" type="int" const="true"/>
 </params>
<code>/*
* This function generates an array of strings representing the labels for the nearest neighbors.
*
* Parameters:
*   - n_neighbors: The number of nearest neighbors.
*
* Returns:
*   - An array of strings representing the labels for the nearest neighbors.
*/

strings S = array(n_neighbors)
loop i=1..n_neighbors
  S[i] = sprintf(&quot;k=%d&quot;, i)
endloop

return S
</code>
</gretl-function>
<gretl-function name="get_scores_by_neighbor" type="matrix" private="1">
 <params count="1">
  <param name="Scores" type="matrices" const="true"/>
 </params>
<code>/*
* This function calculates the mean score for each neighbor in a set of scores.
*
* Parameters:
* - Scores: A set of matrices containing scores for each neighbor. Each column refers
*  to some metric and each row to some fold on which the model was trained on.
*
* Returns:
* - mean_score_by_neighbor: A matrix containing the mean score for each neighbor.
*/

scalar ncols = cols(Scores[1])
scalar npages = nelem(Scores)
strings clabels = cnameget(Scores[1]) # retrieve metric names
matrix mean_score_by_neighbor = mshape(NA, npages, ncols)

loop i=1..ncols
  matrix mean_score_by_neighbor[,i] = meanc(drill(Scores, null, i), TRUE)'
endloop

cnameset(mean_score_by_neighbor, clabels)
rnameset(mean_score_by_neighbor, get_neighbors_label(npages))

return mean_score_by_neighbor
</code>
</gretl-function>
<gretl-function name="knn_compute" type="matrix" private="1">
 <params count="3">
  <param name="self" type="bundle" const="true"/>
  <param name="target" type="matrix" const="true"/>
  <param name="features" type="matrix" const="true"/>
 </params>
<code>/*
* This function computes the k-nearest neighbors (KNN) algorithm.
*
* Parameters:
* - target: The matrix of target variables.
* - features: The matrix of features, eventually standardized
* - self: A bundle containing additional parameters and variables.
* - compute_yhat: A boolean indicating whether to compute the predicted values.
*
* Returns:
* This function does not return any value.
*
* Description:
* The function computes the KNN algorithm by finding the k nearest neighbors for each observation in the features matrix.
* It then computes the predicted values for the target variables based on the type of estimation (regression or classification).
* The computed values are stored in the self.yhat matrix if compute_yhat is set to TRUE.
*/

matrix distances = compute_distances(features, self.distance_type)
# Compute the indices for each observation which are the closest to it
matrix indices_of_closest = indices_of_closest(distances, self.n_neighbors, FALSE)

# Compute the predictions for the target using data on the closest neighbors
if self.type == &quot;regression&quot;
  matrix yhat = compute_yhat_regression(target, indices_of_closest)
elif self.type == &quot;classification&quot;
  matrix yhat = compute_yhat_classification(target, indices_of_closest, self.class_prediction)
else
  errorif(TRUE, &quot;unknown estimation class&quot;)
endif

return yhat
</code>
</gretl-function>
<gretl-function name="knnCV" type="void" private="1">
 <params count="3">
  <param name="target" type="matrix" const="true"/>
  <param name="features" type="matrix" const="true"/>
  <param name="self" type="bundleref"/>
 </params>
<code>/*
* This function performs k-fold cross-validation for the k-nearest neighbors (k-NN) algorithm.
* It takes a target matrix and a features matrix as input, along with a bundle containing
* various parameters for the k-NN algorithm. The function splits the data into training and
* validation sets using a specified cross-validation method, and then estimates the k-NN model
* for each training set. Finally, it computes the scores for each validation set and returns
* the results as a matrix.
*
* Parameters:
*    target: The target matrix containing the dependent variable.
*    features: The features matrix containing the independent variables.
*    self: A bundle containing the following parameters for the k-NN algorithm:
*        - kfold_nsplits: The number of folds for cross-validation.
*        - win_size: The window size for the time-series cross-validation method.
*        - splitters: The type of cross-validation method to use.
*        - stdize_features: Whether to standardize the features before fitting the model.
*        - type: The type of k-NN algorithm to use (e.g., classification or regression).
*        - class_prediction: The type of class prediction to use for classification.
*        - n_neighbors: The number of nearest neighbors to consider.
*        - distance_type: The type of distance metric to use.
*
* Returns:
*    The scores matrix, which contains the evaluation scores for each validation set.
*/

# Preparations for CvDataSplitter
bundle DataCV = null
matrix DataCV.X = target ~ features
matrix DataCV.index = seq(1, rows(DataCV.X))'
scalar DataCV.n_folds = self.kfold_nsplits
scalar DataCV.win_size = self.win_size
string DataCV.cv_type = self.splitters
# Retrive the training and validation data
CvDataSplitter(&amp;DataCV)
scalar self.n_training_sets = nelem(DataCV.X_train)

# Estimate the model for each training set
matrix Scores = {}

loop tset=1..self.n_training_sets
  bundle model = _(stdize_features = self.stdize_features, type = self.type, class_prediction = self.class_prediction, n_neighbors = self.n_neighbors, distance_type = self.distance_type)
  matrix y_train = DataCV.X_train[tset][,2]  # 1st column refers to index
  matrix X_train = DataCV.X_train[tset][,3:]
  matrix y_test = DataCV.X_test[tset][,2] # 1st column refers to index
  matrix X_test = DataCV.X_test[tset][,3:]
  assert(rows(y_train) == rows(X_train))
  assert(rows(y_test) == rows(X_test))

  # set computing in-sample predictions to FALSE as not needed here
  model.features = prepare_features(X_train, self.stdize_features)
  model.target = target
  # predict using the validation set
  matrix forecast = knn_predict(model, X_test)
  matrix Scores ~= knn_scores(y_test, forecast, model)
  strings metric_labels = tset == 1 ? rnameget(Scores) : metric_labels
endloop

self.Scores = Scores'
rnameset(self.Scores, get_folds_label(tset))
cnameset(self.Scores, metric_labels)
</code>
</gretl-function>
<gretl-function name="get_optimal_score_and_k" type="bundle" private="1">
 <params count="3">
  <param name="mean_scores" type="matrix" const="true"/>
  <param name="metric" type="string" const="true"/>
  <param name="type" type="string" const="true"/>
 </params>
<code>/* Based on cross-validated mean scores, retrieve the optimal number of
neighbors based on some metric. */

catch scalar position_criteria = instrings(cnameget(mean_scores), toupper(metric))
errorif($error, &quot;Unknown metric name passed.&quot;)

bundle B = empty
if type == &quot;classification&quot;
  matrix corrected_mean_scores = correct_class_metrics(mean_scores[,position_criteria], metric)
else
  matrix corrected_mean_scores = mean_scores[,position_criteria]
endif

scalar B.optimal_k = iminc(corrected_mean_scores, TRUE)
scalar B.optimal_score = NA

if !ok(B.optimal_k)
  printf &quot;ERROR: Cannot compute optimal no. of neighbors for metric '%s'.\n&quot;, metric
  print &quot;Please select another metric instead.&quot;
  return B
endif

scalar B.optimal_score = mean_scores[B.optimal_k, position_criteria]

return B
</code>
</gretl-function>
<gretl-function name="fcstats_majority" type="matrix" private="1">
 <params count="2">
  <param name="actual" type="matrix" const="true"/>
  <param name="prediction" type="matrix" const="true"/>
 </params>
<code>/**
* Compute evaluation statistics for binary integer outcomes using the majority rule.
*
* actual: The matrix of true actual values.
* prediction: The matrix of predicted values.
* return: The matrix of evaluation statistics, including false rate, hit rate, and Kuipers score.
*/

matrix scores = scores2x2(actual ~ prediction, FALSE)

return scores
</code>
</gretl-function>
<gretl-function name="fcstats_probability" type="matrix" private="1">
 <params count="2">
  <param name="actual" type="matrix" const="true"/>
  <param name="prediction" type="matrix" const="true"/>
 </params>
<code>/*
* This function computes evaluation statistics for binary classification models.
*
* Parameters:
* - actual: A matrix containing the actual class labels.
* - prediction: A matrix containing the predicted probabilities for each class.
*
* Returns:
* - fcstats: A matrix containing the evaluation statistics (computed by the &quot;FEP&quot; package).
*   - If the number of unique values in 'actual' is 2, the column vector
*     contains the following statistics:
*     - Quadratic probability score (QPS)
*     - Logarithmic probability score (LPS)
*   - If the number of unique values in 'actual' is not 2, a warning message is printed and an empty matrix is returned.
*/

if nelem(values(actual)) == 2
  matrix fcstats = probscore(actual, prediction)
  strings label = cnameget(fcstats)
  fcstats = fcstats'
  rnameset(fcstats, label)
else
  printf &quot;\nWARNING: No support for statistics for more than three class outcomes.\n\n&quot;
  matrix fcstats = {}
endif

return fcstats
</code>
</gretl-function>
<gretl-function name="prepare_features" type="matrix" private="1">
 <params count="2">
  <param name="features" type="matrix"/>
  <param name="stdize_features" type="bool" const="true"/>
 </params>
<code>/*
* This function prepares the features matrix for k-nearest neighbors (KNN) algorithm.
*
* Parameters:
*   - features: A matrix containing the features.
*   - stdize_features: A boolean indicating whether to standardize the features.
*
* Returns:
*   - A matrix containing the prepared features.
*/
if stdize_features &amp;&amp; rows(features) &gt; 1
  matrix features = stdize(features, 0, TRUE)
endif

return features
</code>
</gretl-function>
<gretl-function name="get_rsq" type="scalar" private="1">
 <params count="2">
  <param name="target" type="matrix" const="true"/>
  <param name="yhat" type="matrix" const="true"/>
 </params>
<code>/*
* Calculates the R-squared value between the target and predicted values.
*
* target: The matrix of target values.
* yhat: The matrix of predicted values.
* return: The R-squared value.
*/
scalar rsq = mcorr(target~yhat)[1,2]^2
return rsq
</code>
</gretl-function>
<gretl-function name="compute_yhat_regression" type="matrix" private="1">
 <params count="2">
  <param name="target" type="matrix" const="true"/>
  <param name="indices_of_closest" type="matrix" const="true"/>
 </params>
<code>/*
* Compute the predicted values (yhat) based on the target matrix and
* the indices of the closest neighbors for regressions.
*
* target: The target matrix containing the true values.
* indices_of_closest: The matrix containing the indices of the closest neighbors.
*
* return: The matrix containing the predicted values (yhat).
*/

matrix yhat = target_means(target, indices_of_closest)
return yhat
</code>
</gretl-function>
<gretl-function name="compute_yhat_classification" type="matrix" private="1">
 <params count="3">
  <param name="target" type="matrix" const="true"/>
  <param name="indices_of_closest" type="matrix" const="true"/>
  <param name="class_prediction" type="string" const="true"/>
 </params>
<code>/*
* Compute the predicted values (yhat) based on the target matrix and
* the indices of the closest neighbors for classification.
*
* target: The target matrix containing the true values.
* indices_of_closest: The matrix containing the indices of the closest neighbors.
*
* return: The matrix containing the predicted values (yhat).
*/

matrix yhat = target_class(target, indices_of_closest, class_prediction)

return yhat
</code>
</gretl-function>
<gretl-function name="target_means" type="matrix" private="1">
 <params count="2">
  <param name="target" type="matrix" const="true"/>
  <param name="indices_of_closest" type="matrix" const="true"/>
 </params>
<code>/*
* Calculates the mean values of the target variable from the training data
* for each observation of the closest neighbors (measured relative to the test data
* record).
*
* target: The matrix containing the target variable values from the training data.
* indices_of_closest: The matrix containing the indices of the closest
neighbors for each observation.

* return: The matrix of mean values of the target variable for each observation.
*/

scalar N = cols(indices_of_closest)
matrix yhat = mshape(NA, N, 1)

loop i=1..N
  matrix idx = indices_of_closest[,i]
  matrix yhat[i] = mean(target[idx])
endloop

return yhat
</code>
</gretl-function>
<gretl-function name="target_class" type="matrix" private="1">
 <params count="3">
  <param name="target" type="matrix" const="true"/>
  <param name="indices_of_closest" type="matrix" const="true"/>
  <param name="class_prediction" type="string" const="true"/>
 </params>
<code>/*
* This function calculates either the mode or conditional probability of the target
* variable for each observation based on the indices of the closest neighbors.
*
* Parameters:
* - target: A matrix containing the target variable values for all observations.
* - indices_of_closest: A matrix containing the indices of the closest
*   neighbors for each observation.
* - class_prediction: Type of prediction; either `majority` or `probability`.
*
* Returns:
* - yhat: A matrix containing either the mode or conditional probability of
*   the target variable for each observation.
*/

scalar N = cols(indices_of_closest)
matrix yhat = mshape(NA, N, 1)
scalar information = class_prediction == &quot;majority&quot; ? 1 : 2

loop i = 1..N
  matrix idx = indices_of_closest[,i]
  matrix value = onemode(target[idx])[information]

  if ok(value)
    matrix yhat[i] = value
  else
    printf &quot;WARNING: Could not compute prediction for observation %d. Ignore.\n&quot;, $i
  endif
endloop

return yhat
</code>
</gretl-function>
<gretl-function name="indices_of_closest" type="matrix" private="1">
 <params count="3">
  <param name="distances" type="matrix" const="true"/>
  <param name="n_neighbors" type="int" const="true"/>
  <param name="outofsample" type="bool" default="1" const="true"/>
 </params>
<code>/*
This function calculates the indices of the closest neighbors based on the given distances.

Parameters:
- distances: matrix, the distances between observations. The rows refer to
each testdata record. The columns refer to the distance of each testdata
observation to each point of the training set
- n_neighbors: int, the number of closest neighbors to consider.

Returns:
- idx: matrix, the indices of the closest neighbors (in rows) for each
testdata record (in columns). The rows equal the number of neighbors.
*/

scalar n_testdata = rows(distances)
matrix idx = seq(1, cols(distances))'
matrix indices_of_closest = mshape(NA, n_neighbors, n_testdata)

# TODO: Check whether this can be improved in terms of computation
# TODO: In principle this may be parallelized but not in this version

loop i = 1..n_testdata
  matrix row = distances[i,]' ~ idx   # retrieve the i-th test observation
  matrix sorted = msortby(row, 1)
  if outofsample == FALSE
    sorted = sorted[-i,]  # remove reference observation (distance to itself)
  endif

  matrix indices_of_closest[,i] = sorted[1:n_neighbors, 2]
endloop

return indices_of_closest
</code>
</gretl-function>
<gretl-function name="correct_class_metrics" type="matrix" private="1">
 <params count="2">
  <param name="Scores" type="matrix" const="true"/>
  <param name="metric" type="string" const="true"/>
 </params>
<code>/* This function computes the correct classification metrics by adjusting the
sign of the scores based on predefined criteria. It is designed to work within
the context of classification tasks, where different metrics might need to be
interpreted differently (e.g., a higher score might be better for some metrics
but worse for others). */

bundle Map = _(POD = &quot;negative&quot;, POFD = &quot;unchanged&quot;, HR = &quot;negative&quot;, FAR = &quot;unchanged&quot;, CSI = &quot;negative&quot;, OR = &quot;negative&quot;, BIAS = &quot;negative&quot;, TSS = &quot;negative&quot;, HSS = &quot;negative&quot;, ETS = &quot;unchanged&quot;, PRC = &quot;negative&quot;, FSC = &quot;negative&quot;, QPS = &quot;negative&quot;, LPS = &quot;negative&quot;)

scalar pos = inbundle(Map, metric)
errorif(!ok(pos), &quot;Unknown metric name passed.&quot;)

matrix ret = Scores .* (Map[metric] == &quot;positive&quot; ? 1 : -1)

return ret
</code>
</gretl-function>
<gretl-function name="get_metric_name" type="string" private="1">
 <params count="1">
  <param name="self" type="bundle" const="true"/>
 </params>
<code>if self.type == &quot;classification&quot;
  string metric = self.scoring_classification
else
  string metric = self.scoring_regression
endif

return metric
</code>
</gretl-function>
<gretl-function name="check_metric_probclass" type="void" private="1">
 <params count="3">
  <param name="type" type="string" const="true"/>
  <param name="metric" type="string" const="true"/>
  <param name="class_prediction" type="string" const="true"/>
 </params>
<code>if type == &quot;classification&quot;
  if metric == &quot;QPS&quot; &amp;&amp; class_prediction != &quot;probability&quot;
    printf &quot;\nERROR: The metric '%s' is only supported for probability-based classification.\n&quot;, metric
    errorif(TRUE, sprintf(&quot;Please set 'class_prediction' to 'probability' to use this metric.\n\n&quot;))
  elif class_prediction == &quot;probability&quot; &amp;&amp; (metric != &quot;QPS&quot; &amp;&amp; metric != &quot;LPS&quot;)
    errorif(TRUE, &quot;For binary classification, only 'QPS' and 'LPS' are supported. Please use another metric&quot;)
  endif
endif
</code>
</gretl-function>
<gretl-function name="errorif_multiclass_prob" type="void" private="1">
 <params count="3">
  <param name="y" type="series" const="true"/>
  <param name="class_prediction" type="string" const="true"/>
  <param name="type" type="string" const="true"/>
 </params>
<code>errorif(type == &quot;classification&quot; &amp;&amp; nelem(values(y)) &gt; 2 &amp;&amp; class_prediction == &quot;probability&quot;, &quot;Probabilities are only supported for binary classification, yet.&quot;)
</code>
</gretl-function>
<gretl-function name="errorif_classification_loo" type="void" private="1">
 <params count="2">
  <param name="type" type="string" const="true"/>
  <param name="splitters" type="string" const="true"/>
 </params>
<code>errorif(splitters == &quot;loo&quot; &amp;&amp; type == &quot;classification&quot;, &quot;Leave-one-out CV (loo) not supportet for classification, yet. Please use 'kfold', instead.&quot;)
</code>
</gretl-function>
<sample-script>
clear
set verbose off
include knn.gfn
set seed 1234

/*
include &quot;/home/artur/git/knn/src/knn.inp&quot; --force
include CvDataSplitter.gfn
include extra.gfn
include FEP.gfn
*/


open credscore.gdt --quiet

# Remove missing values
smpl --no-missing --permanent
# Define arbitrary training set
genr index
series trainset = (index &lt;= 50) ? TRUE : FALSE

# Parameters
N_NEIGHBORS = 3   # no. of neighbors (fixed)

# Select an example to run
EXAMPLE = 4


if EXAMPLE == 1   # regression
    list x = Age MDR OwnRent
    series y = Income
    smpl trainset == TRUE --restrict
    bundle Model = knn_fit(y, x, N_NEIGHBORS)

    knn_summary(Model)  # Print summary of estimation
    series yhat = Model.yhat
    series uhat = Model.uhat
    print y yhat uhat --byobs --range=1:10

    print &quot;In-sample scores&quot;
    print knn_scores(y, yhat, Model)

    # in-sample prediction
    series prediction = knn_predict(Model, x)
    summary y prediction --simple

elif EXAMPLE == 2  # binary classification with majority voting
    list x = Age Income
    series y = Acc
    setinfo y --discrete
    bundle Model = knn_fit(y, x, N_NEIGHBORS)

    knn_summary(Model)  # Print summary of estimation
    series yhat = Model.yhat
    series uhat = Model.uhat
    print y yhat uhat --byobs --range=1:10
    print &quot;In-sample scores&quot;
    print knn_scores(y, yhat, Model)

    # in-sample prediction
    series prediction = knn_predict(Model, x)
    summary y prediction --simple


elif EXAMPLE == 3
    #####################################
    ### Regression using cross-validation for
    ### computing the optimal number of neighbors
    ### Run k-fold sampling with 5 folds
    #####################################
    list x = Age MDR OwnRent
    series y = Income

    # Set parameters
    matrix NEIGHBORS = seq(1,4)  # sequence of no. of neighbors to evaluate
    scalar KFOLD_NSPLITS = 5     # only relevant for splitters = &quot;kfold&quot;
    string SPLITTERS = &quot;kfold&quot;   # either &quot;kfold&quot;, &quot;loo&quot;
    string METRIC = &quot;mae&quot;        # metric to optimize, default: rmse
    bundle Params = _(splitters = SPLITTERS,
                      kfold_nsplits = KFOLD_NSPLITS,
                      scoring_regression = METRIC)

    # Set training set: just some random sample (not stratified!)
    series trainset = randgen(i, 0, 1) &lt;= 0.5 ? TRUE : FALSE
    smpl trainset == TRUE --restrict
    bundle Model = knn_fit(y, x, NEIGHBORS, Params)
    print Model
    knn_summary(Model)
    knn_plot_score(Model)
    knn_plot_cvscores(Model)

    # Activate testset and predict out-of-sample
    smpl trainset == FALSE --restrict --replace
    series prediction = knn_predict(Model, x)
    # Out-of-sample scores
    print &quot;Out-of-sample scores&quot;
    print knn_scores(y, prediction, Model)
    print y prediction -o --range=:15


elif EXAMPLE == 4
    #####################################
    ### 3-class classification using cross-validation for
    ### computing the optimal number of neighbors
    ### Run k-fold sampling with 5 folds
    #####################################
    list x = MDR OwnRent Acc
    # Construct target with three arbitrary classes
    series y = 0
    y = Age &gt;= 30 ? 1 : y
    y = Age &gt;= 40 ? 2 : y
    setinfo y --discrete  # tell gretl that's a categorical variable

    # Set parameters
    matrix NEIGHBORS = seq(1,4)  # sequence of no. of neighbors to evaluate
    scalar KFOLD_NSPLITS = 5     # only relevant for splitters = &quot;kfold&quot;
    string SPLITTERS = &quot;kfold&quot;   # either &quot;kfold&quot;, &quot;loo&quot;
    string METRIC = &quot;PRC&quot;        # metric to optimize, default: rmse
    bundle Params = _(splitters = SPLITTERS,
                      kfold_nsplits = KFOLD_NSPLITS,
                      scoring_regression = METRIC)

    # Set training set: just some random sample (not stratified!)
    series trainset = randgen(i, 0, 1) &lt;= 0.5 ? TRUE : FALSE
    smpl trainset == TRUE --restrict
    bundle Model = knn_fit(y, x, NEIGHBORS, Params)
    print Model
    knn_summary(Model)
    knn_plot_cvscores(Model)

    # Activate testset and predict out-of-sample
    smpl trainset == FALSE --restrict --replace
    series prediction = knn_predict(Model, x)
    print y prediction -o --range=:15
endif
</sample-script>
</gretl-function-package>
</gretl-functions>
